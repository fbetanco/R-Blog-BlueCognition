[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/ML/index.html#section-2.1---data-cleaning",
    "href": "posts/ML/index.html#section-2.1---data-cleaning",
    "title": "Spotify Songs Streaming Prediction",
    "section": "Section 2.1 - Data Cleaning",
    "text": "Section 2.1 - Data Cleaning\nLet’s see what is in the dataset train file and do some data wrangling as needed and avoiding data leakage.\nFirst, I found that the author of the data describe the deifnition of each feature in the dataset:\ntrack_name: Name of the song artist(s)\nname: Name of the artist(s) of the song\nartist_count: Number of artists contributing to the song\nreleased_year: Year when the song was released\nreleased_month: Month when the song was released\nreleased_day: Day of the month when the song was released\nin_spotify_playlists: Number of Spotify playlists the song is included in\nin_spotify_charts: Presence and rank of the song on Spotify charts\nstreams: Total number of streams on Spotify\nin_apple_playlists: Number of Apple Music playlists the song is included\nin in_apple_charts: Presence and rank of the song on Apple Music charts\nin_deezer_playlists: Number of Deezer playlists the song is included in\nin_deezer_charts: Presence and rank of the song on Deezer charts\nin_shazam_charts: Presence and rank of the song on Shazam charts\nbpm: Beats per minute, a measure of song tempo\nkey: Key of the song\nmode: Mode of the song (major or minor)\ndanceability_%: Percentage indicating how suitable the song is for dancing\nvalence_%: Positivity of the song’s musical content\nenergy_%: Perceived energy level of the song\nacousticness_%: Amount of acoustic sound in the song\ninstrumentalness_%: Amount of instrumental content in the song\nliveness_%: Presence of live performance elements\nspeechiness_%: Amount of spoken words in the song\n\n#Structure and summary of the data\n\nstr(spotify)\n\n'data.frame':   953 obs. of  24 variables:\n $ track_name          : chr  \"Seven (feat. Latto) (Explicit Ver.)\" \"LALA\" \"vampire\" \"Cruel Summer\" ...\n $ artist.s._name      : chr  \"Latto, Jung Kook\" \"Myke Towers\" \"Olivia Rodrigo\" \"Taylor Swift\" ...\n $ artist_count        : int  2 1 1 1 1 2 2 1 1 2 ...\n $ released_year       : int  2023 2023 2023 2019 2023 2023 2023 2023 2023 2023 ...\n $ released_month      : int  7 3 6 8 5 6 3 7 5 3 ...\n $ released_day        : int  14 23 30 23 18 1 16 7 15 17 ...\n $ in_spotify_playlists: int  553 1474 1397 7858 3133 2186 3090 714 1096 2953 ...\n $ in_spotify_charts   : int  147 48 113 100 50 91 50 43 83 44 ...\n $ streams             : chr  \"141381703\" \"133716286\" \"140003974\" \"800840817\" ...\n $ in_apple_playlists  : int  43 48 94 116 84 67 34 25 60 49 ...\n $ in_apple_charts     : int  263 126 207 207 133 213 222 89 210 110 ...\n $ in_deezer_playlists : chr  \"45\" \"58\" \"91\" \"125\" ...\n $ in_deezer_charts    : int  10 14 14 12 15 17 13 13 11 13 ...\n $ in_shazam_charts    : chr  \"826\" \"382\" \"949\" \"548\" ...\n $ bpm                 : int  125 92 138 170 144 141 148 100 130 170 ...\n $ key                 : chr  \"B\" \"C#\" \"F\" \"A\" ...\n $ mode                : chr  \"Major\" \"Major\" \"Major\" \"Major\" ...\n $ danceability_.      : int  80 71 51 55 65 92 67 67 85 81 ...\n $ valence_.           : int  89 61 32 58 23 66 83 26 22 56 ...\n $ energy_.            : int  83 74 53 72 80 58 76 71 62 48 ...\n $ acousticness_.      : int  31 7 17 11 14 19 48 37 12 21 ...\n $ instrumentalness_.  : int  0 0 0 0 63 0 0 0 0 0 ...\n $ liveness_.          : int  8 10 31 11 11 8 8 11 28 8 ...\n $ speechiness_.       : int  4 4 6 15 6 24 3 4 9 33 ...\n\nsummary(spotify)\n\n  track_name        artist.s._name      artist_count   released_year \n Length:953         Length:953         Min.   :1.000   Min.   :1930  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.:2020  \n Mode  :character   Mode  :character   Median :1.000   Median :2022  \n                                       Mean   :1.556   Mean   :2018  \n                                       3rd Qu.:2.000   3rd Qu.:2022  \n                                       Max.   :8.000   Max.   :2023  \n released_month    released_day   in_spotify_playlists in_spotify_charts\n Min.   : 1.000   Min.   : 1.00   Min.   :   31        Min.   :  0.00   \n 1st Qu.: 3.000   1st Qu.: 6.00   1st Qu.:  875        1st Qu.:  0.00   \n Median : 6.000   Median :13.00   Median : 2224        Median :  3.00   \n Mean   : 6.034   Mean   :13.93   Mean   : 5200        Mean   : 12.01   \n 3rd Qu.: 9.000   3rd Qu.:22.00   3rd Qu.: 5542        3rd Qu.: 16.00   \n Max.   :12.000   Max.   :31.00   Max.   :52898        Max.   :147.00   \n   streams          in_apple_playlists in_apple_charts  in_deezer_playlists\n Length:953         Min.   :  0.00     Min.   :  0.00   Length:953         \n Class :character   1st Qu.: 13.00     1st Qu.:  7.00   Class :character   \n Mode  :character   Median : 34.00     Median : 38.00   Mode  :character   \n                    Mean   : 67.81     Mean   : 51.91                      \n                    3rd Qu.: 88.00     3rd Qu.: 87.00                      \n                    Max.   :672.00     Max.   :275.00                      \n in_deezer_charts in_shazam_charts        bpm            key           \n Min.   : 0.000   Length:953         Min.   : 65.0   Length:953        \n 1st Qu.: 0.000   Class :character   1st Qu.:100.0   Class :character  \n Median : 0.000   Mode  :character   Median :121.0   Mode  :character  \n Mean   : 2.666                      Mean   :122.5                     \n 3rd Qu.: 2.000                      3rd Qu.:140.0                     \n Max.   :58.000                      Max.   :206.0                     \n     mode           danceability_.    valence_.        energy_.    \n Length:953         Min.   :23.00   Min.   : 4.00   Min.   : 9.00  \n Class :character   1st Qu.:57.00   1st Qu.:32.00   1st Qu.:53.00  \n Mode  :character   Median :69.00   Median :51.00   Median :66.00  \n                    Mean   :66.97   Mean   :51.43   Mean   :64.28  \n                    3rd Qu.:78.00   3rd Qu.:70.00   3rd Qu.:77.00  \n                    Max.   :96.00   Max.   :97.00   Max.   :97.00  \n acousticness_.  instrumentalness_.   liveness_.    speechiness_.  \n Min.   : 0.00   Min.   : 0.000     Min.   : 3.00   Min.   : 2.00  \n 1st Qu.: 6.00   1st Qu.: 0.000     1st Qu.:10.00   1st Qu.: 4.00  \n Median :18.00   Median : 0.000     Median :12.00   Median : 6.00  \n Mean   :27.06   Mean   : 1.581     Mean   :18.21   Mean   :10.13  \n 3rd Qu.:43.00   3rd Qu.: 0.000     3rd Qu.:24.00   3rd Qu.:11.00  \n Max.   :97.00   Max.   :91.000     Max.   :97.00   Max.   :64.00  \n\n\nI will make some obvious adjustments to certain variables, for example the number of “streams” have to be numeric, for some reason it is set as character. Also I’ll fix some features names in the dataset.\n\n# Mutating to numeric variables that should be numeric (streams\n\nspotify$streams &lt;- as.numeric(spotify$streams)\n\nWarning: NAs introduced by coercion\n\nspotify$in_deezer_playlists &lt;- as.numeric(spotify$in_deezer_playlists)\n\nWarning: NAs introduced by coercion\n\nspotify$in_shazam_charts &lt;- as.numeric(spotify$in_shazam_charts)\n\nWarning: NAs introduced by coercion\n\n#I'll remove special characters from Artist name and track name. \n\nspotify &lt;- spotify %&gt;% mutate(artist.s._name = str_remove_all(artist.s._name, \"[^[:alnum:]]\"))\nspotify &lt;- spotify %&gt;% mutate(track_name = str_remove_all(track_name, \"[^[:alnum:]]\"))\n\n# Now I'll create an ID column by merging these 2 columns\n\nspotify$id &lt;- paste(spotify$track_name, spotify$artist.s._name)\n\n#Eliminating spaces\n\nspotify &lt;- spotify %&gt;% mutate(id = str_remove_all(id, \"[^[:alnum:]]\"))\n\n#Renaming some features names to facilitate the coding later\n\nspotify &lt;- rename(spotify, artist.name = artist.s._name, number.art.song = artist_count, danceability = danceability_., valence = valence_., energy = energy_., acousticness = acousticness_., instrumentalness = instrumentalness_., liveness = liveness_., speechiness = speechiness_.)\n\n\n#Let's check how the df looks now\ndim(spotify)\n\n[1] 953  25\n\nsummary(spotify)\n\n  track_name        artist.name        number.art.song released_year \n Length:953         Length:953         Min.   :1.000   Min.   :1930  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.:2020  \n Mode  :character   Mode  :character   Median :1.000   Median :2022  \n                                       Mean   :1.556   Mean   :2018  \n                                       3rd Qu.:2.000   3rd Qu.:2022  \n                                       Max.   :8.000   Max.   :2023  \n                                                                     \n released_month    released_day   in_spotify_playlists in_spotify_charts\n Min.   : 1.000   Min.   : 1.00   Min.   :   31        Min.   :  0.00   \n 1st Qu.: 3.000   1st Qu.: 6.00   1st Qu.:  875        1st Qu.:  0.00   \n Median : 6.000   Median :13.00   Median : 2224        Median :  3.00   \n Mean   : 6.034   Mean   :13.93   Mean   : 5200        Mean   : 12.01   \n 3rd Qu.: 9.000   3rd Qu.:22.00   3rd Qu.: 5542        3rd Qu.: 16.00   \n Max.   :12.000   Max.   :31.00   Max.   :52898        Max.   :147.00   \n                                                                        \n    streams          in_apple_playlists in_apple_charts  in_deezer_playlists\n Min.   :2.762e+03   Min.   :  0.00     Min.   :  0.00   Min.   :  0.0      \n 1st Qu.:1.416e+08   1st Qu.: 13.00     1st Qu.:  7.00   1st Qu.: 12.0      \n Median :2.905e+08   Median : 34.00     Median : 38.00   Median : 36.5      \n Mean   :5.141e+08   Mean   : 67.81     Mean   : 51.91   Mean   :109.7      \n 3rd Qu.:6.739e+08   3rd Qu.: 88.00     3rd Qu.: 87.00   3rd Qu.:110.0      \n Max.   :3.704e+09   Max.   :672.00     Max.   :275.00   Max.   :974.0      \n NA's   :1                                               NA's   :79         \n in_deezer_charts in_shazam_charts      bpm            key           \n Min.   : 0.000   Min.   :  0.00   Min.   : 65.0   Length:953        \n 1st Qu.: 0.000   1st Qu.:  0.00   1st Qu.:100.0   Class :character  \n Median : 0.000   Median :  2.00   Median :121.0   Mode  :character  \n Mean   : 2.666   Mean   : 51.18   Mean   :122.5                     \n 3rd Qu.: 2.000   3rd Qu.: 36.00   3rd Qu.:140.0                     \n Max.   :58.000   Max.   :953.00   Max.   :206.0                     \n                  NA's   :57                                         \n     mode            danceability      valence          energy     \n Length:953         Min.   :23.00   Min.   : 4.00   Min.   : 9.00  \n Class :character   1st Qu.:57.00   1st Qu.:32.00   1st Qu.:53.00  \n Mode  :character   Median :69.00   Median :51.00   Median :66.00  \n                    Mean   :66.97   Mean   :51.43   Mean   :64.28  \n                    3rd Qu.:78.00   3rd Qu.:70.00   3rd Qu.:77.00  \n                    Max.   :96.00   Max.   :97.00   Max.   :97.00  \n                                                                   \n  acousticness   instrumentalness    liveness      speechiness   \n Min.   : 0.00   Min.   : 0.000   Min.   : 3.00   Min.   : 2.00  \n 1st Qu.: 6.00   1st Qu.: 0.000   1st Qu.:10.00   1st Qu.: 4.00  \n Median :18.00   Median : 0.000   Median :12.00   Median : 6.00  \n Mean   :27.06   Mean   : 1.581   Mean   :18.21   Mean   :10.13  \n 3rd Qu.:43.00   3rd Qu.: 0.000   3rd Qu.:24.00   3rd Qu.:11.00  \n Max.   :97.00   Max.   :91.000   Max.   :97.00   Max.   :64.00  \n                                                                 \n      id           \n Length:953        \n Class :character  \n Mode  :character"
  },
  {
    "objectID": "posts/ML/index.html#section-2.2---numerical-and-visual-summary",
    "href": "posts/ML/index.html#section-2.2---numerical-and-visual-summary",
    "title": "Spotify Songs Streaming Prediction",
    "section": "Section 2.2 - Numerical and Visual Summary",
    "text": "Section 2.2 - Numerical and Visual Summary\nLet’s split the dataset, explore more the data to do some wrangling and visualize some key variables in the dataset.\n\n# splitting dataset to have classifications to test. I will randomly split the dataset into 80% for training and 20% for testing\nset.seed(197)\nran &lt;- sample(1:nrow(spotify), 0.8 * nrow(spotify)) \n\n\n# Split the data into training and testing sets\ntrain &lt;- spotify[ran,]\ntest &lt;- spotify[-ran,]\n\nChecking the dimensions of the new datasets\n\ndim(train)\n\n[1] 762  25\n\ndim(test)\n\n[1] 191  25\n\n\nTraining set has 762 rows/observations and 38 features or variables, while the test set contain 191 observations and the same 38 variables.\nI’ll do some data wrangling for the train set\n\n# Let's review the character variables and create dummies that can be use later when building the models\n\nunique(train$key)\n\n [1] \"F#\" \"G\"  \"D\"  \"G#\" \"A#\" \"E\"  \"C#\" \"\"   \"A\"  \"D#\" \"F\"  \"B\" \n\nunique(train$mode)\n\n[1] \"Minor\" \"Major\"\n\n# Exploring the frequency of the values in the key\n\nkey.table&lt;-table(train$key)\nkey.table\n\n\n    A A#  B C#  D D#  E  F F#  G G# \n74 61 43 65 94 62 28 51 69 64 82 69 \n\n#replacing blank cells with NA\n\ntrain$key &lt;- na_if(train$key, '')\n\n#replacing NA with the most frequent value in the feature, in this case C#\n\ntrain &lt;- train %&gt;% \n  mutate(key = ifelse(is.na(key), \"C#\", key))\n\n#I'll create dummy for key and mode \n\ntrain &lt;- train %&gt;% \n  mutate(mode.n = case_when(\n        mode == \"Major\" ~ 1,\n        mode == \"Minor\" ~ 0,\n        ))%&gt;%\n  mutate(key.A = case_when(\n        key == \"A\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.B = case_when(\n        key == \"B\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.D = case_when(\n        key == \"D\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.E = case_when(\n        key == \"E\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.F = case_when(\n        key == \"F\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.G = case_when(\n        key == \"G\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.CS = case_when(\n        key == \"C#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.FS = case_when(\n        key == \"F#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.GS = case_when(\n        key == \"G#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.AS = case_when(\n        key == \"A#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.DS = case_when(\n        key == \"D#\" ~ 1,\n        TRUE ~ 0))\n\n#Lastly I'll replace the some NAs using median. For in_deezer_playlists, in_shazam_charts and stream\n\nmedian.stream.t &lt;- median(train$streams, na.rm = TRUE)\nmedian.deezer.playlist.t &lt;- median(train$in_deezer_playlists, na.rm = TRUE)\nmedian.shazam.chart.t &lt;- median(train$in_shazam_charts, na.rm = TRUE)\n\ntrain &lt;- train %&gt;% mutate(streams = ifelse(is.na(streams), median.stream.t, streams)) %&gt;%\n                   mutate(in_deezer_playlists = ifelse(is.na(in_deezer_playlists), median.deezer.playlist.t, in_deezer_playlists)) %&gt;%\n                   mutate(in_shazam_charts = ifelse(is.na(in_shazam_charts), median.shazam.chart.t, in_shazam_charts))\n\nsummary(train)\n\n  track_name        artist.name        number.art.song released_year \n Length:762         Length:762         Min.   :1.000   Min.   :1930  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.:2020  \n Mode  :character   Mode  :character   Median :1.000   Median :2022  \n                                       Mean   :1.539   Mean   :2018  \n                                       3rd Qu.:2.000   3rd Qu.:2022  \n                                       Max.   :8.000   Max.   :2023  \n released_month    released_day   in_spotify_playlists in_spotify_charts\n Min.   : 1.000   Min.   : 1.00   Min.   :   31.0      Min.   :  0.00   \n 1st Qu.: 3.000   1st Qu.: 6.00   1st Qu.:  853.8      1st Qu.:  0.00   \n Median : 5.000   Median :13.00   Median : 2167.0      Median :  3.00   \n Mean   : 6.004   Mean   :13.86   Mean   : 5188.8      Mean   : 11.86   \n 3rd Qu.: 9.000   3rd Qu.:22.00   3rd Qu.: 5412.0      3rd Qu.: 15.75   \n Max.   :12.000   Max.   :31.00   Max.   :52898.0      Max.   :147.00   \n    streams          in_apple_playlists in_apple_charts  in_deezer_playlists\n Min.   :2.762e+03   Min.   :  0.00     Min.   :  0.00   Min.   :  0.00     \n 1st Qu.:1.397e+08   1st Qu.: 13.00     1st Qu.:  7.00   1st Qu.: 13.00     \n Median :2.867e+08   Median : 34.50     Median : 38.00   Median : 35.00     \n Mean   :5.051e+08   Mean   : 66.10     Mean   : 52.61   Mean   : 97.88     \n 3rd Qu.:6.573e+08   3rd Qu.: 83.75     3rd Qu.: 87.00   3rd Qu.: 93.00     \n Max.   :3.563e+09   Max.   :537.00     Max.   :275.00   Max.   :974.00     \n in_deezer_charts in_shazam_charts      bpm             key           \n Min.   : 0.000   Min.   :  0.0    Min.   : 65.00   Length:762        \n 1st Qu.: 0.000   1st Qu.:  0.0    1st Qu.: 98.25   Class :character  \n Median : 0.000   Median :  2.0    Median :121.00   Mode  :character  \n Mean   : 2.554   Mean   : 50.1    Mean   :122.36                     \n 3rd Qu.: 2.000   3rd Qu.: 31.0    3rd Qu.:140.00                     \n Max.   :58.000   Max.   :953.0    Max.   :206.00                     \n     mode            danceability      valence          energy     \n Length:762         Min.   :23.00   Min.   : 4.00   Min.   : 9.00  \n Class :character   1st Qu.:57.00   1st Qu.:32.00   1st Qu.:54.00  \n Mode  :character   Median :69.00   Median :51.00   Median :66.00  \n                    Mean   :66.69   Mean   :51.50   Mean   :64.22  \n                    3rd Qu.:78.00   3rd Qu.:70.75   3rd Qu.:77.00  \n                    Max.   :95.00   Max.   :97.00   Max.   :97.00  \n  acousticness   instrumentalness    liveness      speechiness   \n Min.   : 0.00   Min.   : 0.000   Min.   : 3.00   Min.   : 2.00  \n 1st Qu.: 6.00   1st Qu.: 0.000   1st Qu.:10.00   1st Qu.: 4.00  \n Median :17.00   Median : 0.000   Median :12.00   Median : 6.00  \n Mean   :27.42   Mean   : 1.682   Mean   :18.12   Mean   :10.12  \n 3rd Qu.:44.00   3rd Qu.: 0.000   3rd Qu.:23.00   3rd Qu.:11.00  \n Max.   :97.00   Max.   :91.000   Max.   :91.00   Max.   :64.00  \n      id                mode.n          key.A             key.B       \n Length:762         Min.   :0.000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.000   Median :0.00000   Median :0.0000  \n                    Mean   :0.584   Mean   :0.08005   Mean   :0.0853  \n                    3rd Qu.:1.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n                    Max.   :1.000   Max.   :1.00000   Max.   :1.0000  \n     key.D             key.E             key.F             key.G       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.0000  \n Mean   :0.08136   Mean   :0.06693   Mean   :0.09055   Mean   :0.1076  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.0000  \n     key.CS           key.FS            key.GS            key.AS       \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.2205   Mean   :0.08399   Mean   :0.09055   Mean   :0.05643  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n     key.DS       \n Min.   :0.00000  \n 1st Qu.:0.00000  \n Median :0.00000  \n Mean   :0.03675  \n 3rd Qu.:0.00000  \n Max.   :1.00000  \n\ndim(train)\n\n[1] 762  37\n\n\nNow let’s explore the training dataset with focus on Streams which is the target variable\n\n#Let's see a summary of the target variable (number of streams)\n\nsummary.stream &lt;- descr(train$streams)\nprint(summary.stream)\n\nDescriptive Statistics  \ntrain$streams  \nN: 762  \n\n                          streams\n----------------- ---------------\n             Mean    505056426.77\n          Std.Dev    555606450.79\n              Min         2762.00\n               Q1    139681964.00\n           Median    286739476.00\n               Q3    657723613.00\n              Max   3562543890.00\n              MAD    275729763.36\n              IQR    517575700.75\n               CV            1.10\n         Skewness            1.98\n      SE.Skewness            0.09\n         Kurtosis            4.12\n          N.Valid          762.00\n        Pct.Valid          100.00\n\n\nGiven the difference between Mean and Median (~2x) and curtosis value (&gt;0 - 4.12), it sounds like a distribution far to a normal distribution with high probability of extreme values.\nLet’s visualize the distribution. I’ll use a density plot.\n\n# Density estimation\ndensity_streams &lt;- density(train$streams)\ndensity_spot_play &lt;- density(train$in_spotify_playlists)\ndensity_app_play &lt;- density(train$in_apple_playlists)\ndensity_dee_play &lt;- density(train$in_deezer_playlists)\ndensity_acc &lt;- density(train$acousticness)\n\n# Plot the density curve\nplot(density_streams, type = \"l\", main = \"Density Plot for Streams\", xlab = \"Values\", ylab = \"Density\")\n\n\n\n\n\n\n\nplot(density_spot_play, type = \"l\", main = \"Density Plot for In Spotify Playlist\", xlab = \"Values\", ylab = \"Density\")\n\n\n\n\n\n\n\nplot(density_app_play, type = \"l\", main = \"Density Plot for In Apple Playlist\", xlab = \"Values\", ylab = \"Density\")\n\n\n\n\n\n\n\nplot(density_dee_play, type = \"l\", main = \"Density Plot for In Deezer Playlist\", xlab = \"Values\", ylab = \"Density\")\n\n\n\n\n\n\n\nplot(density_acc, type = \"l\", main = \"Density Plot for Acousticness\", xlab = \"Values\", ylab = \"Density\")\n\n\n\n\n\n\n\n\nCertainly the distribution seems with a very heavy tail towards the higher values. Also looking at some key variables\nIt seems that just a few artist has a very high volume of streams represented by extreme values.\nJust curious about the streams by artist:\n\n#Let's create a table summarizing streams by artist\n\ndt.train &lt;- data.table(train)\ndt.train2 &lt;- dt.train[,list(Total.streams = sum(streams, na.rm=T), freq = .N), by = c(\"artist.name\")]\n\nStream_table &lt;- dt.train2 %&gt;% \n  group_by(artist.name) %&gt;%\n  summarise(Total.streams = sum(Total.streams, na.rm=TRUE), Streams.Median = median(Total.streams, na.rm=TRUE))\nStream_table &lt;- Stream_table %&gt;%\n  mutate(Total.Streams.Percent = Total.streams/(sum(Total.streams))*100.2)\nStream_table &lt;- Stream_table[with (Stream_table, order(-Total.Streams.Percent)),]\n\nStream_table &lt;- Stream_table%&gt;%\n  mutate(Cum_Percent = cumsum(Total.Streams.Percent))\n\ndata_subset &lt;- slice(Stream_table, 1:30)\n\n# Print the subset of the data as a nice table using kable\nkable(data_subset)\n\n\n\n\n\nartist.name\nTotal.streams\nStreams.Median\nTotal.Streams.Percent\nCum_Percent\n\n\n\n\nEdSheeran\n12411186494\n12411186494\n3.2313660\n3.231366\n\n\nTaylorSwift\n10812098960\n10812098960\n2.8150289\n6.046395\n\n\nHarryStyles\n9098362425\n9098362425\n2.3688419\n8.415237\n\n\nTheWeeknd\n8957974292\n8957974292\n2.3322906\n10.747527\n\n\nBadBunny\n5925299832\n5925299832\n1.5427060\n12.290233\n\n\nOliviaRodrigo\n4889343765\n4889343765\n1.2729854\n13.563219\n\n\nImagineDragons\n4434404750\n4434404750\n1.1545379\n14.717757\n\n\nBrunoMars\n4185733280\n4185733280\n1.0897940\n15.807551\n\n\nTheNeighbourhood\n4010009939\n4010009939\n1.0440428\n16.851593\n\n\nArcticMonkeys\n3781480286\n3781480286\n0.9845430\n17.836136\n\n\nDojaCat\n3659726247\n3659726247\n0.9528432\n18.788980\n\n\nEminem\n3254582526\n3254582526\n0.8473603\n19.636340\n\n\nDuaLipa\n3227639000\n3227639000\n0.8403454\n20.476685\n\n\nLewisCapaldi\n3126653123\n3126653123\n0.8140528\n21.290738\n\n\nOneRepublic\n3097149603\n3097149603\n0.8063712\n22.097109\n\n\nAdele\n3035946717\n3035946717\n0.7904365\n22.887546\n\n\nLinkinPark\n2985590613\n2985590613\n0.7773258\n23.664872\n\n\nBTS\n2944237123\n2944237123\n0.7665591\n24.431431\n\n\nTonesandI\n2864791672\n2864791672\n0.7458747\n25.177306\n\n\nSZA\n2843515815\n2843515815\n0.7403354\n25.917641\n\n\nPostMaloneSwaeLee\n2808096550\n2808096550\n0.7311136\n26.648755\n\n\nJustinBieber\n2752482785\n2752482785\n0.7166341\n27.365389\n\n\nKendrickLamar\n2689179073\n2689179073\n0.7001524\n28.065541\n\n\nJamesArthur\n2686344050\n2686344050\n0.6994143\n28.764955\n\n\nTheChainsmokersHalsey\n2591224264\n2591224264\n0.6746490\n29.439604\n\n\nTheWeekndDaftPunk\n2565529693\n2565529693\n0.6679591\n30.107563\n\n\nGlassAnimals\n2557975762\n2557975762\n0.6659924\n30.773556\n\n\nShawnMendesCamilaCabello\n2484812918\n2484812918\n0.6469438\n31.420500\n\n\nMne\n2450538862\n2450538862\n0.6380202\n32.058520\n\n\nJoji\n2357270185\n2357270185\n0.6137369\n32.672257\n\n\n\n\n\n\n\n\nIt is interesting to see that almost 10% of total streams are concentrated in 4 artist: Ed Sheeran, Tylor Swift, Harry Styles and The Weeknd. And 30% of all streams comes from 26 artists.\nIt is important to mention that here “artist” means unique artist or combination of artists per song, for example, if Taylor Swift has a song along with Ed Sheeran, then it will be considered a unique artist in the analysis.\n\n#how many unique artist are here\nn_distinct(train$artist.name)\n\n[1] 536\n\n\nWe have 536 “unique” artist, so following previous observation we can say that 5% of the artist (26 artists) concentrate 30% of all streams.\nSomething that I think would be interesting is segmenting the data set in Deciles to create categories of artist from the highest to lowest streamer.\nThen we can use those categories as a classification target.\n\n#Let's create the Decile variable based on the streams to use it later in classification method\n\ntrain &lt;- train %&gt;% mutate(class.decile = ntile(streams, 10))\n\nNow let’s explore a few other features vs streams\n\n#Box plot for Strems and Mode, Key and Number of Artists in the song\n\nbox1 &lt;- ggplot(train, aes(x =mode, y = streams))+\n  geom_boxplot(alpha=0.7, outlier.shape = NA)+\n  facet_wrap(.~number.art.song, scales = \"free\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  theme(legend.position = \"right\") +\n  coord_cartesian(ylim =  c(1e+7, 1e+9))+\n  labs(title=\"Streams vs Mode\",\n       subtitle = \"Grouped by Number of Artist in the Song\",\n        x =\"Mode\", y = \"Streams\")\nbox1\n\n\n\n\n\n\n\nbox2 &lt;- ggplot(train, aes(x =key, y = streams))+\n  geom_boxplot(alpha=0.7, outlier.shape = NA)+\n  facet_wrap(.~number.art.song, scales = \"free\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  theme(legend.position = \"right\") +\n  coord_cartesian(ylim =  c(1e+7, 1.5e+9))+\n  labs(title=\"Streams vs Key\",\n       subtitle = \"Grouped by Number of Artist in the Song\",\n        x =\"Key\", y = \"Streams\")\nbox2\n\n\n\n\n\n\n\n\nIt is interesting to see that there is not too much variability when comparing streams by Mode and Number of Artist in the song, but when comparing among Keys seems that the variability increases when the song has 2 or 3 artists involved.\nLet’s now look at numerical variables using correlation matrix and scatter plot.\n\n#I'll create a dataset with only numeric variables, also I will exclude from here class.decile because is redundnat with streams\n\ntrain.n &lt;- train %&gt;% select(-track_name, -artist.name, -key, -mode, -id, -class.decile)\n\n# Cor Matrix\ncor_matrix1 &lt;- cor(train.n)\ncor_matrix1 &lt;- round(cor_matrix1, 2)\n\n# Let's see a color matrix to simplify visualization\n\ncorrplot(cor_matrix1, method = \"circle\", tl.cex = 0.5)\n\n\n\n\n\n\n\n\nI can see that there is correlation among presence in charts and playlists among the different platforms, in particular seems there is a strong correlation between Spotify and Apple Music playlists and among the charts in all platforms. Here could be some milticolinearity that I may need to pay attention.\nOn the other hand there seems also some multicolinearity among song characteristics (danceability, energy, valence, acousticness, instrumentalness, liveness, speachness), but none of them seems highly correlated to streams. Also noted strong inverse correlation between acousticness and energy).\nLastly, seems that number of streams is highly and positive correlated to the number of playlists the song is included in any of the platforms (spotify, apple and deezer).\nLet’s visualize Streams and playlists in scatter plots\n\nscatter1 &lt;- ggplot(train, aes(in_deezer_playlists,streams))+\n  geom_point()+\n  scale_y_continuous(limits=c(1e+8,1.5e+9))+\n  labs(title=\"Scatter Plot - Streams vs Deezer playlist precense\",\n        x =\"In Deezer Playlist\", y = \"Streams\")\nscatter1\n\nWarning: Removed 175 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nscatter2 &lt;- ggplot(train, aes(in_apple_playlists,streams))+\n  geom_point()+\n  scale_y_continuous(limits=c(1e+8,1.5e+9))+\n  labs(title=\"Scatter Plot - Streams vs Apple playlist precense\",\n        x =\"In Apple Music Playlist\", y = \"Streams\")\nscatter2\n\nWarning: Removed 175 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nscatter3 &lt;- ggplot(train, aes(in_spotify_playlists,streams))+\n  geom_point()+\n  scale_y_continuous(limits=c(1e+8,1.5e+9))+\n  labs(title=\"Scatter Plot - Streams vs Spotify playlist precense\",\n        x =\"In Spotify Playlist\", y = \"Streams\")\nscatter3\n\nWarning: Removed 175 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/ML/index.html#section-4.1---data-preprocessing",
    "href": "posts/ML/index.html#section-4.1---data-preprocessing",
    "title": "Spotify Songs Streaming Prediction",
    "section": "Section 4.1 - Data preprocessing",
    "text": "Section 4.1 - Data preprocessing\n\nMethod 1 - Linear Regression\n\n#Linear regression with all features\nmodel1 &lt;- lm(streams ~ ., data = train.n)\nsummary(model1)\n\n\nCall:\nlm(formula = streams ~ ., data = train.n)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.467e+09 -1.336e+08 -3.137e+07  9.722e+07  2.190e+09 \n\nCoefficients: (1 not defined because of singularities)\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -8.558e+09  2.096e+09  -4.083 4.94e-05 ***\nnumber.art.song      -3.182e+07  1.279e+07  -2.487  0.01311 *  \nreleased_year         4.334e+06  1.039e+06   4.170 3.42e-05 ***\nreleased_month        2.940e+06  3.022e+06   0.973  0.33095    \nreleased_day          3.147e+06  1.153e+06   2.730  0.00648 ** \nin_spotify_playlists  3.793e+04  2.047e+03  18.530  &lt; 2e-16 ***\nin_spotify_charts     4.781e+06  7.712e+05   6.200 9.45e-10 ***\nin_apple_playlists    2.384e+06  2.048e+05  11.640  &lt; 2e-16 ***\nin_apple_charts      -2.736e+05  2.625e+05  -1.042  0.29755    \nin_deezer_playlists   4.687e+05  7.101e+04   6.600 7.88e-11 ***\nin_deezer_charts     -8.956e+06  2.387e+06  -3.752  0.00019 ***\nin_shazam_charts     -4.752e+05  9.508e+04  -4.998 7.25e-07 ***\nbpm                   1.812e+04  3.808e+05   0.048  0.96206    \ndanceability          3.906e+05  8.622e+05   0.453  0.65064    \nvalence              -6.102e+05  5.438e+05  -1.122  0.26225    \nenergy               -7.588e+05  8.644e+05  -0.878  0.38032    \nacousticness          9.046e+05  5.141e+05   1.760  0.07890 .  \ninstrumentalness     -1.709e+06  1.188e+06  -1.438  0.15076    \nliveness             -1.696e+05  7.826e+05  -0.217  0.82847    \nspeechiness          -1.247e+06  1.086e+06  -1.149  0.25107    \nmode.n                4.972e+06  2.275e+07   0.219  0.82710    \nkey.A                 1.095e+07  6.603e+07   0.166  0.86837    \nkey.B                -6.133e+07  6.561e+07  -0.935  0.35027    \nkey.D                -7.028e+07  6.661e+07  -1.055  0.29171    \nkey.E                -5.650e+06  6.776e+07  -0.083  0.93357    \nkey.F                -4.791e+07  6.494e+07  -0.738  0.46090    \nkey.G                -8.322e+07  6.341e+07  -1.312  0.18977    \nkey.CS               -7.180e+07  5.961e+07  -1.204  0.22881    \nkey.FS               -9.925e+07  6.553e+07  -1.515  0.13031    \nkey.GS               -3.300e+07  6.500e+07  -0.508  0.61177    \nkey.AS                3.077e+07  7.020e+07   0.438  0.66133    \nkey.DS                       NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 284800000 on 731 degrees of freedom\nMultiple R-squared:  0.7477,    Adjusted R-squared:  0.7373 \nF-statistic: 72.21 on 30 and 731 DF,  p-value: &lt; 2.2e-16\n\n\nOut of the 31 features only 10 show a significant relationship with the number of streams, most of them around presence in playlists or charts in the different platforms, only one is related to the music compositions (acousticness).\nI’ll try other liner models:\n\n#I'll log the target to see if the prediction improves\nmodel2 &lt;- lm(log(streams) ~ number.art.song + released_year + released_day + in_spotify_playlists + in_spotify_charts + in_apple_playlists + in_deezer_playlists + in_deezer_charts + in_shazam_charts + acousticness, data = train.n)\nsummary(model2)\n\n\nCall:\nlm(formula = log(streams) ~ number.art.song + released_year + \n    released_day + in_spotify_playlists + in_spotify_charts + \n    in_apple_playlists + in_deezer_playlists + in_deezer_charts + \n    in_shazam_charts + acousticness, data = train.n)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9204  -0.4216   0.0697   0.5158   2.0350 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           1.820e+01  5.802e+00   3.138  0.00177 ** \nnumber.art.song      -8.926e-02  3.492e-02  -2.556  0.01078 *  \nreleased_year         3.179e-04  2.872e-03   0.111  0.91191    \nreleased_day          8.862e-03  3.253e-03   2.724  0.00660 ** \nin_spotify_playlists  5.709e-05  5.663e-06  10.081  &lt; 2e-16 ***\nin_spotify_charts     2.409e-03  1.998e-03   1.205  0.22841    \nin_apple_playlists    3.415e-03  5.457e-04   6.259 6.51e-10 ***\nin_deezer_playlists   1.585e-03  2.002e-04   7.918 8.66e-15 ***\nin_deezer_charts     -9.397e-03  6.756e-03  -1.391  0.16467    \nin_shazam_charts     -4.756e-04  2.626e-04  -1.811  0.07052 .  \nacousticness          1.387e-04  1.142e-03   0.121  0.90340    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8143 on 751 degrees of freedom\nMultiple R-squared:  0.5021,    Adjusted R-squared:  0.4955 \nF-statistic: 75.75 on 10 and 751 DF,  p-value: &lt; 2.2e-16\n\n\nActually the model deteriorates significantly when logging streams variable (Adjusted R2=.5 vs .73 with model 1).\nA third model keeping only significant features founded in model 1:\n\n#Let's keep only the significant features\n\nmodel3 &lt;- lm(streams ~ number.art.song + released_year + released_day + in_spotify_playlists + in_spotify_charts + in_apple_playlists + in_deezer_playlists + in_deezer_charts + in_shazam_charts + acousticness, data = train.n)\nsummary(model3)\n\n\nCall:\nlm(formula = streams ~ number.art.song + released_year + released_day + \n    in_spotify_playlists + in_spotify_charts + in_apple_playlists + \n    in_deezer_playlists + in_deezer_charts + in_shazam_charts + \n    acousticness, data = train.n)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.520e+09 -1.330e+08 -3.618e+07  9.146e+07  2.143e+09 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -9.335e+09  2.032e+09  -4.595 5.08e-06 ***\nnumber.art.song      -3.621e+07  1.223e+07  -2.961 0.003159 ** \nreleased_year         4.670e+06  1.006e+06   4.643 4.06e-06 ***\nreleased_day          2.793e+06  1.139e+06   2.452 0.014437 *  \nin_spotify_playlists  3.770e+04  1.983e+03  19.011  &lt; 2e-16 ***\nin_spotify_charts     4.274e+06  6.997e+05   6.108 1.61e-09 ***\nin_apple_playlists    2.357e+06  1.911e+05  12.334  &lt; 2e-16 ***\nin_deezer_playlists   4.690e+05  7.009e+04   6.691 4.33e-11 ***\nin_deezer_charts     -9.241e+06  2.366e+06  -3.906 0.000102 ***\nin_shazam_charts     -4.775e+05  9.195e+04  -5.193 2.67e-07 ***\nacousticness          1.199e+06  4.000e+05   2.996 0.002823 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 285100000 on 751 degrees of freedom\nMultiple R-squared:  0.7401,    Adjusted R-squared:  0.7366 \nF-statistic: 213.8 on 10 and 751 DF,  p-value: &lt; 2.2e-16\n\n\nThe power of explanation when pulling out non-relevant features (based on the statistical significance) practically does not change.\n\n# Make predictions for new data\npredicted_1 &lt;- predict(model1, train.n)\n\nWarning in predict.lm(model1, train.n): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\npredicted_2 &lt;- predict(model2, train.n)\npredicted_3 &lt;- predict(model3, train.n)\n\n#MdAE      \n\nmdae_lm1 &lt;- mdae(train.n$streams, predicted_1)\nmdae_lm2 &lt;- mdae(train.n$streams, predicted_2)\nmdae_lm3 &lt;- mdae(train.n$streams, predicted_3)\n\nprint(mdae_lm1)\n\n[1] 119534826\n\nprint(mdae_lm2)\n\n[1] 286739457\n\nprint(mdae_lm3)\n\n[1] 116079107\n\n\nSeems that model 3 is the best option based on the Median Absolute Error value (lowest error). Logging streams makes the model worst, and interesting to see that model 1 which includes all variables and has the highest R2 has a higher error than model 3 which includes only the most relevant features based on the p value significance.\n\n\nMethod 2 - Lasso Regression\nLasso is particularly useful in situations where feature selection is desired or when the dataset contains many irrelevant or redundant predictors, which seems this case. Also select the most relevant features and reduce coefficient of irrelevant features, which at the same time allows to handle multicollinearity among predictor variables which also seems appropriate given the strong correlation among playlist and chart variables.\n\nsummary(train.n)\n\n number.art.song released_year  released_month    released_day  \n Min.   :1.000   Min.   :1930   Min.   : 1.000   Min.   : 1.00  \n 1st Qu.:1.000   1st Qu.:2020   1st Qu.: 3.000   1st Qu.: 6.00  \n Median :1.000   Median :2022   Median : 5.000   Median :13.00  \n Mean   :1.539   Mean   :2018   Mean   : 6.004   Mean   :13.86  \n 3rd Qu.:2.000   3rd Qu.:2022   3rd Qu.: 9.000   3rd Qu.:22.00  \n Max.   :8.000   Max.   :2023   Max.   :12.000   Max.   :31.00  \n in_spotify_playlists in_spotify_charts    streams          in_apple_playlists\n Min.   :   31.0      Min.   :  0.00    Min.   :2.762e+03   Min.   :  0.00    \n 1st Qu.:  853.8      1st Qu.:  0.00    1st Qu.:1.397e+08   1st Qu.: 13.00    \n Median : 2167.0      Median :  3.00    Median :2.867e+08   Median : 34.50    \n Mean   : 5188.8      Mean   : 11.86    Mean   :5.051e+08   Mean   : 66.10    \n 3rd Qu.: 5412.0      3rd Qu.: 15.75    3rd Qu.:6.573e+08   3rd Qu.: 83.75    \n Max.   :52898.0      Max.   :147.00    Max.   :3.563e+09   Max.   :537.00    \n in_apple_charts  in_deezer_playlists in_deezer_charts in_shazam_charts\n Min.   :  0.00   Min.   :  0.00      Min.   : 0.000   Min.   :  0.0   \n 1st Qu.:  7.00   1st Qu.: 13.00      1st Qu.: 0.000   1st Qu.:  0.0   \n Median : 38.00   Median : 35.00      Median : 0.000   Median :  2.0   \n Mean   : 52.61   Mean   : 97.88      Mean   : 2.554   Mean   : 50.1   \n 3rd Qu.: 87.00   3rd Qu.: 93.00      3rd Qu.: 2.000   3rd Qu.: 31.0   \n Max.   :275.00   Max.   :974.00      Max.   :58.000   Max.   :953.0   \n      bpm          danceability      valence          energy     \n Min.   : 65.00   Min.   :23.00   Min.   : 4.00   Min.   : 9.00  \n 1st Qu.: 98.25   1st Qu.:57.00   1st Qu.:32.00   1st Qu.:54.00  \n Median :121.00   Median :69.00   Median :51.00   Median :66.00  \n Mean   :122.36   Mean   :66.69   Mean   :51.50   Mean   :64.22  \n 3rd Qu.:140.00   3rd Qu.:78.00   3rd Qu.:70.75   3rd Qu.:77.00  \n Max.   :206.00   Max.   :95.00   Max.   :97.00   Max.   :97.00  \n  acousticness   instrumentalness    liveness      speechiness   \n Min.   : 0.00   Min.   : 0.000   Min.   : 3.00   Min.   : 2.00  \n 1st Qu.: 6.00   1st Qu.: 0.000   1st Qu.:10.00   1st Qu.: 4.00  \n Median :17.00   Median : 0.000   Median :12.00   Median : 6.00  \n Mean   :27.42   Mean   : 1.682   Mean   :18.12   Mean   :10.12  \n 3rd Qu.:44.00   3rd Qu.: 0.000   3rd Qu.:23.00   3rd Qu.:11.00  \n Max.   :97.00   Max.   :91.000   Max.   :91.00   Max.   :64.00  \n     mode.n          key.A             key.B            key.D        \n Min.   :0.000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.00000  \n Median :1.000   Median :0.00000   Median :0.0000   Median :0.00000  \n Mean   :0.584   Mean   :0.08005   Mean   :0.0853   Mean   :0.08136  \n 3rd Qu.:1.000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.00000  \n Max.   :1.000   Max.   :1.00000   Max.   :1.0000   Max.   :1.00000  \n     key.E             key.F             key.G            key.CS      \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.06693   Mean   :0.09055   Mean   :0.1076   Mean   :0.2205  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n     key.FS            key.GS            key.AS            key.DS       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.08399   Mean   :0.09055   Mean   :0.05643   Mean   :0.03675  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :1.00000   Max.   :1.00000   Max.   :1.00000   Max.   :1.00000  \n\n#this time I'll separate target variable on a different object because I feel will be easier to handle\n\nstream.train &lt;- train.n$streams\n\n#Now will exclude streams from the dataset and predicted values from linear regression\n\ntrain.n2 &lt;- train.n %&gt;% select(-streams)\n\n#I'll scale train features\n\nxtrain.n2.scaled &lt;- scale(train.n2)\n\n#I'll scale also streams just to used in the cross validation. I rather use scaling as it keeps the distribution\n\ny.train.n2.scaled &lt;- scale(stream.train)\n\n#I'll create the lambda sequence value \n\nlambda.array &lt;- seq(from=0.1, to= 100, by=0.1)\n\n#Cross validation using cv.glmnet\n\ncv.lasso.model &lt;- cv.glmnet(xtrain.n2.scaled, y.train.n2.scaled, alpha=1, lambda=lambda.array)\n\nplot(cv.lasso.model, xvar = 'lambda')\n\nWarning in plot.window(...): \"xvar\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"xvar\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xvar\" is not a\ngraphical parameter\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"xvar\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"xvar\" is not a graphical parameter\n\n\nWarning in title(...): \"xvar\" is not a graphical parameter\n\n\n\n\n\n\n\n\n#getting the best lambda\n\nbest.lasso.lambda &lt;- cv.lasso.model$lambda.min\nbest.lasso.lambda\n\n[1] 0.1\n\n#MSE\nmean.lasso.error &lt;- mean(cv.lasso.model$cvm)\nprint(mean.lasso.error)\n\n[1] 0.9984841\n\nprint(coef(cv.lasso.model))\n\n32 x 1 sparse Matrix of class \"dgCMatrix\"\n                               s1\n(Intercept)          3.050052e-17\nnumber.art.song      .           \nreleased_year        .           \nreleased_month       .           \nreleased_day         .           \nin_spotify_playlists 4.395318e-01\nin_spotify_charts    .           \nin_apple_playlists   3.224483e-01\nin_apple_charts      .           \nin_deezer_playlists  5.549454e-02\nin_deezer_charts     .           \nin_shazam_charts     .           \nbpm                  .           \ndanceability         .           \nvalence              .           \nenergy               .           \nacousticness         .           \ninstrumentalness     .           \nliveness             .           \nspeechiness          .           \nmode.n               .           \nkey.A                .           \nkey.B                .           \nkey.D                .           \nkey.E                .           \nkey.F                .           \nkey.G                .           \nkey.CS               .           \nkey.FS               .           \nkey.GS               .           \nkey.AS               .           \nkey.DS               .           \n\n\nInteresting to see that Lasso model keeps the presence of the song in playlist on the 3 main streaming platforms.\n\n\nMethod 3 - Random Forest\nLastly, I’ll use random forest and this time I’ll use the class.deciles created previously to use it as classification.\n\n#First I'll bring back as a separate object the class.deciles variable\n\ntrain.n2$class.decile &lt;- train$class.decile\ntrain.n2$class.decile &lt;- as.factor(train.n2$class.decile)\n\nTune the hyperparameters:\n\ntune_spec &lt;- rand_forest(trees = tune(), mtry = tune()) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n#starting with this values\n\nrf_grid &lt;- grid_regular(\n  trees(range = c(100, 800)),\n  mtry(range = c(2, 31)),\n  levels = 5)\n\nset.seed(1977)\nfolds &lt;- vfold_cv(train.n2, v = 5)\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(tune_spec) %&gt;%\n  add_formula(class.decile ~ .)\n\n#For the purpose to train the model I'll use F1 as metric, despite I'll use mdae for measuring performance later\n\nmy_metrics &lt;- metric_set(f_meas)\n\nrf_res &lt;- \n  rf_wf %&gt;% \n  tune_grid(resamples = folds, grid = rf_grid, metrics = my_metrics)\n\nVisualize the results and best parameters:\n\nrf_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"f_meas\") %&gt;%\n  select(mtry, trees, mean) %&gt;%\n  ggplot(aes(mtry, trees, fill = mean)) +\n    geom_tile() \n\n\n\n\n\n\n\n\nBest Parameters:\n\nbest_params &lt;- rf_res %&gt;%\n  select_best(\"f_meas\")\n\nbest_params\n\n# A tibble: 1 × 3\n   mtry trees .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    31   275 Preprocessor1_Model22\n\n\nExplore a different region\n\n#Now with this values\n\nrf_grid2 &lt;- grid_regular(\n  trees(range = c(100, 500)),\n  mtry(range = c(10, 31)),\n  levels = 5)\n\nset.seed(1979)\nfolds2 &lt;- vfold_cv(train.n2, v = 10)\n\nrf_res2 &lt;- \n  rf_wf %&gt;% \n  tune_grid(resamples = folds2, grid = rf_grid2, metrics = my_metrics)\n\nrf_res2 %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"f_meas\") %&gt;%\n  select(mtry, trees, mean) %&gt;%\n  ggplot(aes(mtry, trees, fill = mean)) +\n    geom_tile() \n\n\n\n\n\n\n\nbest_params2 &lt;- rf_res2 %&gt;%\n  select_best(\"f_meas\")\n\nbest_params2\n\n# A tibble: 1 × 3\n   mtry trees .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    10   300 Preprocessor1_Model03"
  },
  {
    "objectID": "posts/ML/index.html#section-4.2---choose-hyperparameters-fit-and-test-models",
    "href": "posts/ML/index.html#section-4.2---choose-hyperparameters-fit-and-test-models",
    "title": "Spotify Songs Streaming Prediction",
    "section": "Section 4.2 - Choose hyperparameters; fit and test models",
    "text": "Section 4.2 - Choose hyperparameters; fit and test models\nFirst I’ll make adjustments as training set, to the test set.\n\nsummary(test)\n\n  track_name        artist.name        number.art.song released_year \n Length:191         Length:191         Min.   :1.000   Min.   :1957  \n Class :character   Class :character   1st Qu.:1.000   1st Qu.:2020  \n Mode  :character   Mode  :character   Median :1.000   Median :2022  \n                                       Mean   :1.623   Mean   :2019  \n                                       3rd Qu.:2.000   3rd Qu.:2022  \n                                       Max.   :7.000   Max.   :2023  \n                                                                     \n released_month    released_day   in_spotify_playlists in_spotify_charts\n Min.   : 1.000   Min.   : 1.00   Min.   :   86        Min.   : 0.00    \n 1st Qu.: 3.000   1st Qu.: 6.00   1st Qu.: 1006        1st Qu.: 0.00    \n Median : 6.000   Median :14.00   Median : 2415        Median : 4.00    \n Mean   : 6.152   Mean   :14.22   Mean   : 5245        Mean   :12.59    \n 3rd Qu.: 9.000   3rd Qu.:22.00   3rd Qu.: 5868        3rd Qu.:20.00    \n Max.   :12.000   Max.   :31.00   Max.   :43899        Max.   :83.00    \n                                                                        \n    streams          in_apple_playlists in_apple_charts  in_deezer_playlists\n Min.   :1.365e+06   Min.   :  0.00     Min.   :  0.00   Min.   :  0.0      \n 1st Qu.:1.509e+08   1st Qu.: 13.00     1st Qu.:  8.00   1st Qu.: 14.0      \n Median :3.041e+08   Median : 32.00     Median : 39.00   Median : 39.0      \n Mean   :5.492e+08   Mean   : 74.66     Mean   : 49.09   Mean   :132.8      \n 3rd Qu.:7.416e+08   3rd Qu.:107.00     3rd Qu.: 81.50   3rd Qu.:150.8      \n Max.   :3.704e+09   Max.   :672.00     Max.   :199.00   Max.   :965.0      \n                                                         NA's   :13         \n in_deezer_charts in_shazam_charts      bpm            key           \n Min.   : 0.000   Min.   :  0.00   Min.   : 71.0   Length:191        \n 1st Qu.: 0.000   1st Qu.:  0.00   1st Qu.:101.5   Class :character  \n Median : 0.000   Median :  3.00   Median :120.0   Mode  :character  \n Mean   : 3.115   Mean   : 43.64   Mean   :123.2                     \n 3rd Qu.: 2.000   3rd Qu.: 34.75   3rd Qu.:141.0                     \n Max.   :45.000   Max.   :727.00   Max.   :206.0                     \n                  NA's   :13                                         \n     mode            danceability     valence          energy     \n Length:191         Min.   :25.0   Min.   : 4.00   Min.   :20.00  \n Class :character   1st Qu.:58.0   1st Qu.:35.00   1st Qu.:52.50  \n Mode  :character   Median :70.0   Median :52.00   Median :66.00  \n                    Mean   :68.1   Mean   :51.15   Mean   :64.51  \n                    3rd Qu.:79.0   3rd Qu.:68.00   3rd Qu.:76.50  \n                    Max.   :96.0   Max.   :97.00   Max.   :97.00  \n                                                                  \n  acousticness   instrumentalness    liveness      speechiness   \n Min.   : 0.00   Min.   : 0.000   Min.   : 3.00   Min.   : 2.00  \n 1st Qu.: 6.00   1st Qu.: 0.000   1st Qu.:10.00   1st Qu.: 4.00  \n Median :18.00   Median : 0.000   Median :13.00   Median : 6.00  \n Mean   :25.62   Mean   : 1.178   Mean   :18.59   Mean   :10.19  \n 3rd Qu.:38.50   3rd Qu.: 0.000   3rd Qu.:25.50   3rd Qu.:13.50  \n Max.   :94.00   Max.   :51.000   Max.   :97.00   Max.   :45.00  \n                                                                 \n      id           \n Length:191        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n# Let's review the character variables and create dummies that can be use later when building the models\n\nunique(test$key)\n\n [1] \"F\"  \"G#\" \"B\"  \"D\"  \"G\"  \"A#\" \"E\"  \"C#\" \"\"   \"F#\" \"A\"  \"D#\"\n\nunique(test$mode)\n\n[1] \"Major\" \"Minor\"\n\n# Exploring the frequency of the values in the key\n\nkey.table&lt;-table(test$key)\nkey.table\n\n\n    A A#  B C#  D D#  E  F F#  G G# \n21 14 14 16 26 19  5 11 20  9 14 22 \n\n#replacing blank cells with NA\n\ntest$key &lt;- na_if(test$key, '')\n\n#replacing NA with the most frequent value in the feature, in this case C#\n\ntest &lt;- test %&gt;% \n  mutate(key = ifelse(is.na(key), \"C#\", key))\n\n#I'll create dummy for key and mode \n\ntest &lt;- test %&gt;% \n  mutate(mode.n = case_when(\n        mode == \"Major\" ~ 1,\n        mode == \"Minor\" ~ 0,\n        ))%&gt;%\n  mutate(key.A = case_when(\n        key == \"A\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.B = case_when(\n        key == \"B\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.D = case_when(\n        key == \"D\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.E = case_when(\n        key == \"E\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.F = case_when(\n        key == \"F\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.G = case_when(\n        key == \"G\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.CS = case_when(\n        key == \"C#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.FS = case_when(\n        key == \"F#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.GS = case_when(\n        key == \"G#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.AS = case_when(\n        key == \"A#\" ~ 1,\n        TRUE ~ 0))%&gt;%\n  mutate(key.DS = case_when(\n        key == \"D#\" ~ 1,\n        TRUE ~ 0))\n\n#Lastly I'll replace the some NAs using median. For in_deezer_playlists, in_shazam_charts and stream\n\nmedian.deezer.playlist.ts &lt;- median(test$in_deezer_playlists, na.rm = TRUE)\nmedian.shazam.chart.ts &lt;- median(test$in_shazam_charts, na.rm = TRUE)\n\ntest &lt;- test %&gt;% \n  mutate(in_deezer_playlists = ifelse(is.na(in_deezer_playlists), median.deezer.playlist.t, in_deezer_playlists)) %&gt;%\n  mutate(in_shazam_charts = ifelse(is.na(in_shazam_charts), median.shazam.chart.t, in_shazam_charts))\n\n#Let's create the Decile variable based on the streams to use it later in classification method\n\ntest &lt;- test %&gt;% mutate(class.decile = ntile(streams, 10))\n\n#now the dataset for testing, removing non-numeric or irrelevant variables\n\ntest.n &lt;- test %&gt;% select(-track_name, -artist.name, -key, -mode, -id, -class.decile)\n\n#extracting test actual streams \n\nstream.test &lt;- test.n$streams\n\n\nLinear regression fitting and testing\n\n# Make predictions for new data\nlm.predicted.1 &lt;- predict(model1, test.n)\n\nWarning in predict.lm(model1, test.n): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n\nlm.predicted.3 &lt;- predict(model3, test.n)\n\n#Checking MdAE\nmdae_lm1.final &lt;- mdae(stream.test, lm.predicted.1)\nprint(mdae_lm1.final)\n\n[1] 135837770\n\nmdae_lm3.final &lt;- mdae(stream.test, lm.predicted.3)\nprint(mdae_lm3.final)\n\n[1] 123474556\n\n\n\n\nLasso Regression fitting and testing\n\n#Now will exclude streams from the dataset and predicted values from linear regression\n\ntest.n2 &lt;- test.n %&gt;% select(-streams)\n\n#I'll scale test features\n\nxtest.n2.scaled &lt;- scale(test.n2)\n\n#fit the model using the best lambda in lasso\n\nlasso.model &lt;- glmnet(xtest.n2.scaled, stream.test, alpha=1, lambda=best.lasso.lambda)\n coef(lasso.model)\n\n32 x 1 sparse Matrix of class \"dgCMatrix\"\n                             s0\n(Intercept)           549175763\nnumber.art.song       -14064461\nreleased_year         -28898388\nreleased_month         15279704\nreleased_day           11015034\nin_spotify_playlists  226034156\nin_spotify_charts     151290594\nin_apple_playlists    249601103\nin_apple_charts        21338444\nin_deezer_playlists   114996405\nin_deezer_charts      -69063564\nin_shazam_charts     -126032019\nbpm                    -3506104\ndanceability          -47405242\nvalence                -7346601\nenergy                -18401617\nacousticness          -17181614\ninstrumentalness        7630502\nliveness               14395334\nspeechiness            -4810544\nmode.n                -13167404\nkey.A                 -12077321\nkey.B                   5200964\nkey.D                   5981359\nkey.E                   6473279\nkey.F                  17801687\nkey.G                 -21177792\nkey.CS                 -4135984\nkey.FS                 24496493\nkey.GS                -19115050\nkey.AS                  1662262\nkey.DS                 11780702\n\n#Predict streams in test set\n\nytest.lasso.pred &lt;- predict(lasso.model, newx = xtest.n2.scaled, penalty = best.lasso.lambda)\n\n\n#Checking MdAE\nmdae_lasso.final &lt;- mdae(stream.test, ytest.lasso.pred)\nprint(mdae_lasso.final)\n\n[1] 113556904\n\n\n\n\nRandom Forest fitting and testing\n\n#First I need to bring back class.decile for the test set\n\ntest.n2$class.decile &lt;- test$class.decile\ntest.n2$class.decile &lt;- as.factor(test.n2$class.decile)\ntest.class &lt;- test.n2$class.decile\n\nWill first check model 1 (mtry=31 and trees=275)\n\n#fit the model 1\n\nrf1 &lt;- rand_forest(mtry=31, trees=275) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n\nrf_fit1 &lt;- rf1 %&gt;%\n  fit(class.decile ~ ., data = test.n2)\n\n#Predict on the test set using first parameters:\n\ntest_pred1 &lt;-  rf_fit1 %&gt;%\n  predict(test.n2) %&gt;%\n  bind_cols(test.n2) %&gt;%\n  select(class.decile, .pred_class)\n\ntest_pred1 %&gt;%\n    my_metrics(truth = class.decile, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  macro          0.995\n\n#Transforming to numeric to be able to calculate mdae\n\nrf1.pred &lt;- test_pred1$.pred_class\nrf1.pred &lt;- as.numeric(rf1.pred)\ntest.class.n &lt;- as.numeric(test.class)\n\n#Let's see the consufion matrix\nconfusion_matrix1 &lt;- table(test.class.n, rf1.pred)\nconfusion_matrix1\n\n            rf1.pred\ntest.class.n  1  2  3  4  5  6  7  8  9 10\n          1  20  0  0  0  0  0  0  0  0  0\n          2   0 19  0  0  0  0  0  0  0  0\n          3   0  0 19  0  0  0  0  0  0  0\n          4   0  0  0 19  0  0  0  0  0  0\n          5   0  0  0  0 18  0  0  0  1  0\n          6   0  0  0  0  0 19  0  0  0  0\n          7   0  0  0  0  0  0 19  0  0  0\n          8   0  0  0  0  0  0  0 19  0  0\n          9   0  0  0  0  0  0  0  0 19  0\n          10  0  0  0  0  0  0  0  0  0 19\n\n#Median Absolute Error\n\nmdae_rf1.final &lt;- mdae(test.class.n, rf1.pred)\nprint(mdae_rf1.final)\n\n[1] 0\n\n\nIt is important to mention that Median Absolute Error is not the ideal metric for Random Forest performance, it is more common using Accuracy, F1 Score, Precision, ROC AUC, Recall, etc. MdAE is more appropiate for regression type of method (instead of classification), however if I need to use the same metric for all methods, MdAE is feasible.\nNow model 2 (mtry=10 and trees=300)\n\n#fit the model 2\n\nrf2 &lt;- rand_forest(mtry=10, trees=300) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\n\nrf_fit2 &lt;- rf2 %&gt;%\n  fit(class.decile ~ ., data = test.n2)\n\n#Predict on the test set using first parameters:\n\ntest_pred2 &lt;-  rf_fit2 %&gt;%\n  predict(test.n2) %&gt;%\n  bind_cols(test.n2) %&gt;%\n  select(class.decile, .pred_class)\n\ntest_pred2 %&gt;%\n    my_metrics(truth = class.decile, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  macro          0.990\n\nrf2.pred2 &lt;- test_pred2$.pred_class\nrf2.pred2 &lt;- as.numeric(rf2.pred2)\ntest.class.n2 &lt;- as.numeric(test.class)\n\n#Let's see the consufion matrix\nconfusion_matrix2 &lt;- table(test.class.n, rf2.pred2)\nconfusion_matrix2\n\n            rf2.pred2\ntest.class.n  1  2  3  4  5  6  7  8  9 10\n          1  20  0  0  0  0  0  0  0  0  0\n          2   0 19  0  0  0  0  0  0  0  0\n          3   0  0 19  0  0  0  0  0  0  0\n          4   0  0  0 19  0  0  0  0  0  0\n          5   0  0  0  0 18  0  0  0  1  0\n          6   0  0  0  0  0 18  0  0  1  0\n          7   0  0  0  0  0  0 19  0  0  0\n          8   0  0  0  0  0  0  0 19  0  0\n          9   0  0  0  0  0  0  0  0 19  0\n          10  0  0  0  0  0  0  0  0  0 19\n\n#Median Absolute Error\n\nmdae_rf2.final &lt;- mdae(test.class.n, rf2.pred2)\nprint(mdae_rf2.final)\n\n[1] 0"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R-Blog-BlueCognition",
    "section": "",
    "text": "Network Analysis on subreddit r/politics\n\n\nSocial Network Analysis\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nFelix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nSpotify Songs Streaming Prediction\n\n\n\n\n\n\nspotify\n\n\nmachine learning\n\n\nregression\n\n\nclassification\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nFelix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nNew test post\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 5, 2024\n\n\nFelix Betancourt\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nFelix B\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/my_new_post/index.html",
    "href": "posts/my_new_post/index.html",
    "title": "New test post",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\nHi this is a test"
  },
  {
    "objectID": "posts/Post 4 - Final Project/index.html#final-project-695n---network-analysis-on-subreddit-rpolitics",
    "href": "posts/Post 4 - Final Project/index.html#final-project-695n---network-analysis-on-subreddit-rpolitics",
    "title": "Final Project FB - Post 4 - Final post",
    "section": "Final Project 695N - Network Analysis on subreddit r/politics",
    "text": "Final Project 695N - Network Analysis on subreddit r/politics\n\nIntroduction\nSocial media, including Reddit, serves as a prominent platform for discourse and community engagement. The “Politics” subreddit (r/politics) is a hub for discussions on political matters, attracting users who share news articles and express opinions (Bail, 2016).\nIn recent years, Reddit discussions have intensified, especially regarding U.S. politics, notably around Joe Biden and Donald Trump. Analyzing user interactions in r/politics offers insight into digital political landscapes (Conover et al., 2013).\nThis research aims to explore Reddit users’ connections within r/politics, focusing on Biden and Trump discussions. We seek to uncover community formation patterns and influencers through social network analysis. Specifically, we’ll examine users’ connectivity, community emergence, and the relationship between post popularity and user influence.\nBy addressing these questions, we contribute to understanding political discourse on Reddit and social media’s role in shaping public opinion.\n\n\nResearch Question\n\nHow are Reddit users connected in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nHow are these group of users connected based on the sentiment of their comments?\nIs there a relationship between Score (net value between upvotes and downvotes) for a post, and how the user is connected to other users?\n\n\n\nHypothesis\nThis research is mainly exploratory, I am not expecting something in particular, so I don’t have an specific hypothesis, except for the research question number 4.\nIn relation to the fourth research question I can expect that:\nH1: an user’s higher score should be highly related to a higher centrality in the network.\n\n\nData\n\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n\n\nData Wrangling\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package) and saved the objects generated from the scrapping to csv files\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\ngetwd()\n\n[1] \"C:/Users/fbeta/OneDrive/Blue Cognition/Blog/R-Blog-BlueCognition/posts/Post 4 - Final Project\"\n\n# Read large CSV file using fread\npolitik1 &lt;- fread(\"politics_comments1.csv\")\npolitik2 &lt;- fread(\"politics_comments2.csv\")\npolitik3 &lt;- fread(\"politics_comments3.csv\")\n\n#Checking the structure of the data sets\nglimpse(politik1)\n\nRows: 983\nColumns: 8\n$ V1        &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  &lt;IDate&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ text      &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n\nglimpse(politik2)\n\nRows: 983\nColumns: 16\n$ V1                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                &lt;chr&gt; \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 &lt;chr&gt; \"Supreme Court starts arguments as Biden adminis…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              &lt;dbl&gt; 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n\nglimpse(politik3)\n\nRows: 95,879\nColumns: 11\n$ V1         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     &lt;chr&gt; \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  &lt;int&gt; 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id &lt;chr&gt; \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. “politik3” contain detailed comments on each post and the hierarchical sequence of comments to each post.\nFor the purpose of this research we will define the users or authors to posts and comments as the nodes, and edges are defined as comments made in the same post; it does mean that the network will be undirected as I will consider only authors commenting in the same post but I won’t capture the direction of the comment (B is commenting to A post).\nLet’s do some data wrangling first:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 &lt;- politik3 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments where the author was “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nLet’s see how many nodes (authors/users) we got in this data set:\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n\n[1] 31554\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I’ll select a sample of posts to analyze.\nI’ll select the top 1% posts with more comments.\n\n#first let's see the distribution of number of comments\npercentiles &lt;- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n\n\nLet’s subset the data set with the top 1% posts in terms of comments and let’s see how many posts we have.\n\nsubset_politik2 &lt;- subset(politik_df, comments &gt;= 1439 )\nglimpse(subset_politik2)\n\nRows: 10\nColumns: 14\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                &lt;chr&gt; \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 &lt;chr&gt; \"Letitia James fires back after Donald Trump's b…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              &lt;dbl&gt; 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           &lt;int&gt; 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              &lt;int&gt; 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n\nlength(unique(subset_politik2$author))\n\n[1] 10\n\n\nWe got a data set with 10 original posts and 10 authors, this is now a more “reasonable” data frame to analyze.\nNow I need to identify these post into the “politik_df3” data set which contain all the hierarchical comments network.\n\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 4,902\nColumns: 10\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     &lt;chr&gt; \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id &lt;chr&gt; \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n\n[1] 3869\n\n\nWe got 982 posts but still +3.8k nodes, it is still high number of nodes.\nI’ll need a different approach.\nI’ll select 2 specific posts with a “median” number of comments. One post will be about Trump and another about Biden, specifically I will filter posts by containing the word “Biden” and “Trump” in the title of the post.\n\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n\n#Filtering the titles that contain Trump\ntrump_df &lt;- politik_df %&gt;% filter(grepl(\"Trump\", title))\ntrump_df$candidate &lt;- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump &lt;- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n})\n\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n\n\nThe median comment for Trump’s post is 74 comments.Therefore I’ll select the post with 74 comments.\n\n#Trump\ntrump_post &lt;- subset(trump_df, comments == 74 )\n\nNow let’s select Biden’s post\n\nsuppressWarnings({\n#Filtering the titles that contain Biden\nbiden_df &lt;- politik_df %&gt;% filter(grepl(\"Biden\", title))\nbiden_df$candidate &lt;- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden &lt;- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n\n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n\n\nMedian is 66.5 comments, let’s use the post with 67 comments.\n\n#Biden\nbiden_post &lt;- subset(biden_df, comments == 67 )\n\nNow I got the 2 main posts, let’s explore a bit those 2 posts.\n\n#merging the previous df's\ntrump_biden_df &lt;- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 &lt;- subset_politik3 %&gt;%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 119\nColumns: 11\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       &lt;date&gt; 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"It's very possible that both Biden and Trump are losing so…\n$ comment_id &lt;chr&gt; \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  &lt;chr&gt; \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n\n# Let's keep only the relevant columns\npolitik_final &lt;- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 &lt;- politik_final %&gt;%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level &gt; 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n\n[1] 80\n\nlength(unique(politik_final2$url))\n\n[1] 2\n\n\nNow we got 80 nodes (authors) from the 2 posts.\n\n\nAnalysis\nBefore proceeding with the network analysis, let’s explore a bit about the authors (users).\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_final2 &lt;- politik_final2 %&gt;% mutate(countid = \"1\")\npolitik_final2$countid &lt;- as.numeric(politik_final2$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 &lt;- data.table(politik_final2)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 80 × 2\n   author            Total_posts\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 BrtFrkwr                    7\n 2 betterwoke                  5\n 3 Hattopia                    4\n 4 TheBodyPolitic1             4\n 5 TheRandomInteger            4\n 6 AngusMcTibbins              3\n 7 Due-Shirt616                3\n 8 NoDesinformatziya           3\n 9 TheReddestOrange            3\n10 DarkwingDuckHunt            2\n# ℹ 70 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 80 × 2\n   author             Total_Score\n   &lt;chr&gt;                    &lt;int&gt;\n 1 r-m-russell                 76\n 2 AngusMcTibbins              68\n 3 OverlyComplexPants          40\n 4 BrtFrkwr                    33\n 5 betterwoke                  28\n 6 TheBodyPolitic1             24\n 7 penis_berry_crunch          22\n 8 Due-Shirt616                21\n 9 NoDesinformatziya           20\n10 YeaterdaysQuim              20\n# ℹ 70 more rows\n\n#Score as a proportion of comments\nsummary_score_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_score_per_comment = sum(score)/sum(countid))\nsummary_score_ratio &lt;- summary_score_ratio %&gt;% arrange(desc(Ratio_score_per_comment))\nprint(summary_score_ratio)\n\n# A tibble: 80 × 2\n   author             Ratio_score_per_comment\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 r-m-russell                           76  \n 2 OverlyComplexPants                    40  \n 3 AngusMcTibbins                        22.7\n 4 penis_berry_crunch                    22  \n 5 YeaterdaysQuim                        20  \n 6 fxkatt                                20  \n 7 grixorbatz                            17  \n 8 Sea_Engine4333                        16  \n 9 Quiet_Dimensions                      14  \n10 hdiggyh                               14  \n# ℹ 70 more rows\n\n\nI would expect that nodes (users) with higher score and/or higher score per comment should be predominant (central) in the network.\nFor instance we got the following users in the top 5 in terms of score: “r-m-russell”, “AngusMcTibbins”, “OverlyComplexPants”, “BrtFrkwr” and “betterwoke”.\nOn the other hand here the top 5 users with highest score per comment: “r-m-russell”, “OverlyComplexPants”, “AngusMcTibbins”, “penis_berry_crunch”, and “YeaterdaysQuim”.\nNow I am ready to work on this data set for the Network analysis.\n\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 &lt;- politik_final2 %&gt;%\n  rename(distance = Level)\n\n#identify who is commenting on the same post\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level &lt;- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 &lt;- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 &lt;- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes &lt;- select(politik_final3, c(\"score\", \"candidate\", \"distance\"))\n\nI’ll prepare the adjacency matrix:\n\npolitik_m &lt;- select(politik_final3, c(\"comment_id2\", \"author\"))\n\n# Identify unique names and codes\nunique_names &lt;- unique(politik_final3$author)\nunique_codes &lt;- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix &lt;- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code &lt;- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) &gt; 0) {\n      adj_matrix[unique_names[i], unique_names[j]] &lt;- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) &lt;- 0\n\n\nNetwork Analysis\nNow let’s explore the Network.\nIt is important to mention that in this research we will assume an undirected network. We are only considering comments within the same post, not specific “direction” of each comment among users.\n\n#load packages\nsuppressPackageStartupMessages(library(network))\nlibrary(sna)\nlibrary(statnet)\n\npolitik.n &lt;- network(adj_matrix, directed = FALSE)\npolitik.n\n\n Network attributes:\n  vertices = 80 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 158 \n    missing edges= 0 \n    non-missing edges= 158 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\nWe got 80 nodes and 316 edges.\nLet’s explore the network. I’ll calculate the census for Dyads and triads:\n\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n\n     Mut Asym Null\n[1,] 158    0 3002\n\nsum(sna::triad.census(politik.n))\n\n[1] 82160\n\nsna::triad.census(politik.n)\n\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n\n\nIn terms of Dyads, we got 158 mutual connections and 3002 null connections.\nIn terms of Triads, we got 82160 triads in total: about 70k null triads, 11k open triads (one connection exist), 179 where 2 connections exist, and 337 closed triads.\nIt seems a disperse network, but let’s see Transitivity and Density:\nLet’s check the Transitivity coefficient\n\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n\n[1] 0.8495798\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\nLet’s see the density:\n\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n\n[1] 0.05\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\nLet’s visualize the network\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nWithout isolated nodes and labels\n\n# Plot the network\nplot(politik.n, displaylabels = F, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nLet’s now include the Candidate as attribute. We will clasify the users as per their comments to Biden or Trump posts.\n\n#I'll create a column with Biden true-false attribute\npolitik_final3at &lt;- politik_final3 %&gt;%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how authonrs in Biden and Trump are interacting\nnodeColors&lt;-ifelse(politik_final3at$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=T, main = \"Authors Network by Candidate\") #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nLet’s exclude isolated nodes\n\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=F, main = \"Authors Network by Candidate (excluding isolated nodes)\") #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nIt looks like there are closed communities not connected among them. Which is consistent with the Transitivity vs Density finding before.\nIn particular Biden’s commentors tend to be together and Trump’s commentors seems more disperse.\nLet’s see who are the authors with highest degree centrality\n\n# create a dataset of vertex names and degree: statnet\npolitik.nodes.df &lt;- data.frame(name = politik.n %v% \"vertex.names\",\n                            degree = sna::degree(politik.n))\n\npolitik_table7 &lt;- data.table(politik.nodes.df)\n\n#order by centrality degree\npolitik_table7 %&gt;% arrange(desc(degree)) %&gt;%\nslice(1:10)\n\n                  name degree\n                &lt;char&gt;  &lt;num&gt;\n 1:           Knoxcore     30\n 2:  NoDesinformatziya     30\n 3:           BrtFrkwr     28\n 4:   TheRandomInteger     26\n 5:    TheBodyPolitic1     20\n 6:         Fasefirst2     20\n 7:           cdiddy19     20\n 8: Invincible_auxcord     20\n 9:     Bulky-You-5657     20\n10:        nickmiele22     20\n\nsummary(politik_table7)\n\n     name               degree    \n Length:80          Min.   : 0.0  \n Class :character   1st Qu.: 0.0  \n Mode  :character   Median : 6.0  \n                    Mean   : 7.9  \n                    3rd Qu.:14.5  \n                    Max.   :30.0  \n\n\nLet’s explore the correlation between centrality degree and score (remember it is the net value between upvotes and downvotes).\nI am expecting a significant positive relationship between soore and degree centrality.\n\nsuppressPackageStartupMessages(library(igraph))\n\n# Create the igraph object\npolitik.ig &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")  # Undirected by default\n\n# Calculate degree centrality for each node\ndegree_centrality &lt;- degree(politik.ig, mode = \"all\")\n\n# If nodes do not have names, you can use node IDs\nif (is.null(V(politik.ig)$name)) {\n  V(politik.ig)$name &lt;- as.character(1:vcount(politik.ig))\n}\n\n# Check node names\nnode_names &lt;- V(politik.ig)$name\n\n# Create a sample data frame with some values for each node\n# Ensure the data frame has the same node identifiers as the graph\ndf &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  value = runif(vcount(politik.ig), 1, 100)  # Random values between 1 and 100\n)\n\n# Convert degree centrality to a data frame\ndegree_centrality_df &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  degree_centrality = degree_centrality  # Degree centrality values\n)\n\n# Merge the degree centrality data frame with the existing data frame\nmerged_df &lt;- merge(df, degree_centrality_df, by = \"author\", all = TRUE)\nmerged_df2 &lt;- merge(merged_df, politik_final2, by = \"author\", all = TRUE)\ndegree_scoredf &lt;- select(merged_df2, c(\"degree_centrality\", \"score\"))\n\n# Calculate the correlation coefficient between 'score' and 'degree_centrality'\ncor_matrix1 &lt;- cor(degree_scoredf, use = \"complete.obs\")\ncor_matrix1\n\n                  degree_centrality       score\ndegree_centrality       1.000000000 0.001921923\nscore                   0.001921923 1.000000000\n\n\nSeems that the relationship between score and degree centrality is very low (0.002).\nSo I can’t accept my hypothesis.\nI am curious about the top 5 users with higher degree centrality.\n\ndegree_scoredf3 &lt;- select(merged_df2, c(\"degree_centrality\", \"author\"))\nsubset_df &lt;- distinct(degree_scoredf3, author, .keep_all = TRUE)\n\nsubset_df %&gt;% arrange(desc(degree_centrality))\n\n   degree_centrality               author\n1                 15             Knoxcore\n2                 15    NoDesinformatziya\n3                 14             BrtFrkwr\n4                 13     TheRandomInteger\n5                 10       Bulky-You-5657\n6                 10             cdiddy19\n7                 10  Exciting_Slice_9492\n8                 10           Fasefirst2\n9                 10        hilljack26301\n10                10   Invincible_auxcord\n11                10             Kamelasa\n12                10          nickmiele22\n13                10      TheBodyPolitic1\n14                 8   Admirable_Bad_5649\n15                 8           betterwoke\n16                 8     DarkwingDuckHunt\n17                 8          Scarlettail\n18                 8        Spara-Extreme\n19                 8     TheReddestOrange\n20                 8     Thick-Return1694\n21                 7          bakeacake45\n22                 6            AliMcGraw\n23                 6       AngusMcTibbins\n24                 6             Hattopia\n25                 6    hellocattlecookie\n26                 6   Mundane_Rabbit7751\n27                 6   OverlyComplexPants\n28                 6         Roasted_Butt\n29                 6    SeegsonSynthetics\n30                 6             Shaunair\n31                 4             caserock\n32                 4     CecilTWashington\n33                 4            gefjunhel\n34                 4   penis_berry_crunch\n35                 4          r-m-russell\n36                 3         Due-Shirt616\n37                 3              hdiggyh\n38                 3             iuthnj34\n39                 3               JeffMo\n40                 3        PineTreeBanjo\n41                 3              Purify5\n42                 3     Quiet_Dimensions\n43                 3       YeaterdaysQuim\n44                 1  Candid_Chicken_9246\n45                 1   FijiFanBotNotGay69\n46                 1        Happypappy213\n47                 1            hindusoul\n48                 1      InGreedWeTrust3\n49                 1              LariRed\n50                 1   physical_graffitti\n51                 1            Tower6011\n52                 0              bck1999\n53                 0     bloombergopinion\n54                 0          cryolongman\n55                 0          Donut131313\n56                 0              eldred2\n57                 0      Empty-Rise-4409\n58                 0     fore_skin_walker\n59                 0               fxkatt\n60                 0           grixorbatz\n61                 0            Guttenber\n62                 0        HonoredPeople\n63                 0     ImportantNeck491\n64                 0        inconsistent3\n65                 0       JubalHarshaw23\n66                 0             njman100\n67                 0          No_Yak_6227\n68                 0       Odd_Tiger_2278\n69                 0 platanthera_ciliaris\n70                 0        PoopieButt317\n71                 0  Practical_Shop_4055\n72                 0         RUIN_NATION_\n73                 0       Sea_Engine4333\n74                 0          SeaSuch2077\n75                 0             spotspam\n76                 0          stjoechief1\n77                 0          StormOk7544\n78                 0              syg-123\n79                 0               th1961\n80                 0         Willowgirl78\n\n\nUsers with higher degree centrality are “Knoxcore”, “NoDesinformatziya”, “BrtFrkwr”, “TheRandomInteger”, and “Bulky-You-5657”\nOut of these 5 nodes only one of them (“BrtFrkwr”) is also in the top 5 related to score. So it is consistent with the low correlation between score and centrality.\nLet’s now check what clusters (communities) we do have in this network.\n\n# run clustering algorithm: fast_greedy\npolitik.fg &lt;- igraph::cluster_fast_greedy(politik.ig)\n# inspect clustering object\npolitik.fg\n\nIGRAPH clustering fast greedy, groups: 37, mod: 0.63\n+ groups:\n  $`1`\n   [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n   [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n   [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n  [10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n  [13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n  [16] \"Kamelasa\"           \n  \n  $`2`\n   [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n  + ... omitted several groups/vertices\n\nigraph::groups(politik.fg)\n\n$`1`\n [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n[10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n[13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n[16] \"Kamelasa\"           \n\n$`2`\n [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n [4] \"Roasted_Butt\"        \"BrtFrkwr\"            \"TheRandomInteger\"   \n [7] \"Shaunair\"            \"betterwoke\"          \"DarkwingDuckHunt\"   \n[10] \"Thick-Return1694\"    \"Spara-Extreme\"       \"Admirable_Bad_5649\" \n[13] \"TheReddestOrange\"    \"Scarlettail\"         \"Tower6011\"          \n[16] \"Candid_Chicken_9246\"\n\n$`3`\n[1] \"r-m-russell\"        \"penis_berry_crunch\" \"CecilTWashington\"  \n[4] \"caserock\"           \"gefjunhel\"         \n\n$`4`\n[1] \"iuthnj34\"       \"YeaterdaysQuim\" \"hdiggyh\"        \"Purify5\"       \n\n$`5`\n[1] \"Quiet_Dimensions\" \"Due-Shirt616\"     \"JeffMo\"           \"PineTreeBanjo\"   \n\n$`6`\n[1] \"physical_graffitti\" \"hindusoul\"         \n\n$`7`\n[1] \"LariRed\"         \"InGreedWeTrust3\"\n\n$`8`\n[1] \"Happypappy213\"      \"FijiFanBotNotGay69\"\n\n$`9`\n[1] \"fxkatt\"\n\n$`10`\n[1] \"Sea_Engine4333\"\n\n$`11`\n[1] \"Practical_Shop_4055\"\n\n$`12`\n[1] \"PoopieButt317\"\n\n$`13`\n[1] \"inconsistent3\"\n\n$`14`\n[1] \"Empty-Rise-4409\"\n\n$`15`\n[1] \"stjoechief1\"\n\n$`16`\n[1] \"th1961\"\n\n$`17`\n[1] \"platanthera_ciliaris\"\n\n$`18`\n[1] \"No_Yak_6227\"\n\n$`19`\n[1] \"fore_skin_walker\"\n\n$`20`\n[1] \"HonoredPeople\"\n\n$`21`\n[1] \"bloombergopinion\"\n\n$`22`\n[1] \"RUIN_NATION_\"\n\n$`23`\n[1] \"ImportantNeck491\"\n\n$`24`\n[1] \"grixorbatz\"\n\n$`25`\n[1] \"spotspam\"\n\n$`26`\n[1] \"JubalHarshaw23\"\n\n$`27`\n[1] \"StormOk7544\"\n\n$`28`\n[1] \"njman100\"\n\n$`29`\n[1] \"Odd_Tiger_2278\"\n\n$`30`\n[1] \"cryolongman\"\n\n$`31`\n[1] \"Willowgirl78\"\n\n$`32`\n[1] \"Guttenber\"\n\n$`33`\n[1] \"SeaSuch2077\"\n\n$`34`\n[1] \"syg-123\"\n\n$`35`\n[1] \"bck1999\"\n\n$`36`\n[1] \"Donut131313\"\n\n$`37`\n[1] \"eldred2\"\n\n\nThere are 2 main clusters in the network.\nLet’s see the density of each cluster using block model function.\n\nprint(blockmodel(politik.n, politik.fg$membership)$block.model,\n      digits = 2)\n\n         Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8\nBlock 1     0.62    0.00       0       0       0       0       0       0\nBlock 2     0.00    0.48       0       0       0       0       0       0\nBlock 3     0.00    0.00       1       0       0       0       0       0\nBlock 4     0.00    0.00       0       1       0       0       0       0\nBlock 5     0.00    0.00       0       0       1       0       0       0\nBlock 6     0.00    0.00       0       0       0       1       0       0\nBlock 7     0.00    0.00       0       0       0       0       1       0\nBlock 8     0.00    0.00       0       0       0       0       0       1\nBlock 9     0.00    0.00       0       0       0       0       0       0\nBlock 10    0.00    0.00       0       0       0       0       0       0\nBlock 11    0.00    0.00       0       0       0       0       0       0\nBlock 12    0.00    0.00       0       0       0       0       0       0\nBlock 13    0.00    0.00       0       0       0       0       0       0\nBlock 14    0.00    0.00       0       0       0       0       0       0\nBlock 15    0.00    0.00       0       0       0       0       0       0\nBlock 16    0.00    0.00       0       0       0       0       0       0\nBlock 17    0.00    0.00       0       0       0       0       0       0\nBlock 18    0.00    0.00       0       0       0       0       0       0\nBlock 19    0.00    0.00       0       0       0       0       0       0\nBlock 20    0.00    0.00       0       0       0       0       0       0\nBlock 21    0.00    0.00       0       0       0       0       0       0\nBlock 22    0.00    0.00       0       0       0       0       0       0\nBlock 23    0.00    0.00       0       0       0       0       0       0\nBlock 24    0.00    0.00       0       0       0       0       0       0\nBlock 25    0.00    0.00       0       0       0       0       0       0\nBlock 26    0.00    0.00       0       0       0       0       0       0\nBlock 27    0.00    0.00       0       0       0       0       0       0\nBlock 28    0.00    0.00       0       0       0       0       0       0\nBlock 29    0.00    0.00       0       0       0       0       0       0\nBlock 30    0.00    0.00       0       0       0       0       0       0\nBlock 31    0.00    0.00       0       0       0       0       0       0\nBlock 32    0.00    0.00       0       0       0       0       0       0\nBlock 33    0.00    0.00       0       0       0       0       0       0\nBlock 34    0.00    0.00       0       0       0       0       0       0\nBlock 35    0.00    0.00       0       0       0       0       0       0\nBlock 36    0.00    0.00       0       0       0       0       0       0\nBlock 37    0.00    0.00       0       0       0       0       0       0\n         Block 9 Block 10 Block 11 Block 12 Block 13 Block 14 Block 15 Block 16\nBlock 1        0        0        0        0        0        0        0        0\nBlock 2        0        0        0        0        0        0        0        0\nBlock 3        0        0        0        0        0        0        0        0\nBlock 4        0        0        0        0        0        0        0        0\nBlock 5        0        0        0        0        0        0        0        0\nBlock 6        0        0        0        0        0        0        0        0\nBlock 7        0        0        0        0        0        0        0        0\nBlock 8        0        0        0        0        0        0        0        0\nBlock 9      NaN        0        0        0        0        0        0        0\nBlock 10       0      NaN        0        0        0        0        0        0\nBlock 11       0        0      NaN        0        0        0        0        0\nBlock 12       0        0        0      NaN        0        0        0        0\nBlock 13       0        0        0        0      NaN        0        0        0\nBlock 14       0        0        0        0        0      NaN        0        0\nBlock 15       0        0        0        0        0        0      NaN        0\nBlock 16       0        0        0        0        0        0        0      NaN\nBlock 17       0        0        0        0        0        0        0        0\nBlock 18       0        0        0        0        0        0        0        0\nBlock 19       0        0        0        0        0        0        0        0\nBlock 20       0        0        0        0        0        0        0        0\nBlock 21       0        0        0        0        0        0        0        0\nBlock 22       0        0        0        0        0        0        0        0\nBlock 23       0        0        0        0        0        0        0        0\nBlock 24       0        0        0        0        0        0        0        0\nBlock 25       0        0        0        0        0        0        0        0\nBlock 26       0        0        0        0        0        0        0        0\nBlock 27       0        0        0        0        0        0        0        0\nBlock 28       0        0        0        0        0        0        0        0\nBlock 29       0        0        0        0        0        0        0        0\nBlock 30       0        0        0        0        0        0        0        0\nBlock 31       0        0        0        0        0        0        0        0\nBlock 32       0        0        0        0        0        0        0        0\nBlock 33       0        0        0        0        0        0        0        0\nBlock 34       0        0        0        0        0        0        0        0\nBlock 35       0        0        0        0        0        0        0        0\nBlock 36       0        0        0        0        0        0        0        0\nBlock 37       0        0        0        0        0        0        0        0\n         Block 17 Block 18 Block 19 Block 20 Block 21 Block 22 Block 23\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17      NaN        0        0        0        0        0        0\nBlock 18        0      NaN        0        0        0        0        0\nBlock 19        0        0      NaN        0        0        0        0\nBlock 20        0        0        0      NaN        0        0        0\nBlock 21        0        0        0        0      NaN        0        0\nBlock 22        0        0        0        0        0      NaN        0\nBlock 23        0        0        0        0        0        0      NaN\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 24 Block 25 Block 26 Block 27 Block 28 Block 29 Block 30\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24      NaN        0        0        0        0        0        0\nBlock 25        0      NaN        0        0        0        0        0\nBlock 26        0        0      NaN        0        0        0        0\nBlock 27        0        0        0      NaN        0        0        0\nBlock 28        0        0        0        0      NaN        0        0\nBlock 29        0        0        0        0        0      NaN        0\nBlock 30        0        0        0        0        0        0      NaN\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 31 Block 32 Block 33 Block 34 Block 35 Block 36 Block 37\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31      NaN        0        0        0        0        0        0\nBlock 32        0      NaN        0        0        0        0        0\nBlock 33        0        0      NaN        0        0        0        0\nBlock 34        0        0        0      NaN        0        0        0\nBlock 35        0        0        0        0      NaN        0        0\nBlock 36        0        0        0        0        0      NaN        0\nBlock 37        0        0        0        0        0        0      NaN\n\n\nThe blocks 1 and 2 in our network seems dense.\n\ndf_comm &lt;- data.frame(\n  Node = V(politik.ig)$name,\n  Community = politik.fg$membership,\n  Degree = degree_centrality\n)\n\n# 4. Find maximum degree for each community\n\nhighest_degree_nodes &lt;- df_comm %&gt;%\n  group_by(Community) %&gt;%\n  filter(Degree == max(Degree)) %&gt;%\n  ungroup()\n\nhighest_degree_nodes %&gt;% arrange(-desc(Community))\n\n# A tibble: 51 × 3\n   Node               Community Degree\n   &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Knoxcore                   1     15\n 2 NoDesinformatziya          1     15\n 3 BrtFrkwr                   2     14\n 4 r-m-russell                3      4\n 5 penis_berry_crunch         3      4\n 6 CecilTWashington           3      4\n 7 caserock                   3      4\n 8 gefjunhel                  3      4\n 9 iuthnj34                   4      3\n10 YeaterdaysQuim             4      3\n# ℹ 41 more rows\n\n\nLet’s now visualize the communities including the nodes with highest degree centrality for the main communities (1 and 2):\n\n# Identify nodes with high degree centrality (e.g., top 3)\ntop_nodes &lt;- names(sort(degree_centrality, decreasing = TRUE))[1:5]\n\n# Create a custom labeling vector\nnode_labels &lt;- rep(NA, vcount(politik.ig))\n# Set the labels for the nodes with high degree centrality\nnode_labels[top_nodes] &lt;- top_nodes\n\n# Identify isolated nodes\nisolated_nodes &lt;- which(degree(politik.ig) == 0)\n\n# Remove isolated nodes from the network\nnetwork_no_isolated &lt;- delete_vertices(politik.ig, isolated_nodes)\n\nplot(network_no_isolated,\n     vertex.label = node_labels,  \n     vertex.label.cex = 0.8,\n     vertex.color = membership(politik.fg),\n     vertex.label.color = \"black\",\n     vertex.shape = \"sphere\",\n     layout = layout_with_fr,\n     main = \"Communities without isolated nodes and labeling top 5 central nodes\")\n\n\n\n\n\n\n\n\nIt looks like while we have 2 main communities the nodes with highest degree centrality are mostly in one of the communities.\nIt seems like that group of authors are close among them and at the same time are central in the network.\n\n\nSentiment Analysis and the Network\nLastly let’s add a new attribute to the network using text analysis, in particular sentiment analysis.\nLet’s get the sentiment for each author’s comments.\n\nlibrary(sentimentr)\n\n#labeling based on the sentiment score\ncomments &lt;- politik_final3$comment\nget_sentiment_label &lt;- function(ave_sentiment) {\n  if (ave_sentiment &gt; 0.1) {\n    return(\"Positive\")\n  } else if (ave_sentiment &lt; -0.1) {\n    return(\"Negative\")\n  } else {\n    return(\"Neutral\")\n  }\n}\nsentiment_scores &lt;- sentiment_by(x = comments, text.var = comments)\n\n#adding the label for each author in the data set\npolitik_final3$sentiment &lt;- sapply(sentiment_scores$ave_sentiment, get_sentiment_label)\n\nNow let’s visualize the network by sentiment of each node.\n\n# Create a graph object from the data frame\ng &lt;- graph_from_data_frame(politik.n, directed = FALSE)\n\n# Add node attributes to the graph\nV(g)$sentiment &lt;- politik_final3$sentiment[match(V(g)$name, politik_final3$author)]\n\n\n# Define color palette for categories\ncolor_palette &lt;- c(\"Negative\" = \"red\", \"Neutral\" = \"blue\", \"Positive\" = \"lightgreen\")\n\n# Visualize the network with colored nodes\nplot(g, vertex.color = color_palette[V(g)$sentiment], layout = layout_nicely, vertex.label = NA, vertex.size = 7, isolates=TRUE, main = \"Authors Network by Sentiment\")\nlegend(\"bottomright\", legend = c(\"Positive\", \"Neutral\", \"Negative\"), col = c(\"lightgreen\", \"blue\", \"red\"), pch = c(21, 21), title = \"Node Sentiment\")\n\n\n\n\n\n\n\n\nFrom this chart it looks like:\n\nMost of the sentiments are either neutral or negative\nSentiments tend to get closer, or group among them.\n\n\n\n\n\nConclusion\n\nThe network was found to be sparse (low density) but highly interconnected within subgroups (high transitivity), suggesting the presence of distinct communities.\nThe visualization of the network confirmed the presence of communities, showing separate clusters of users commenting on Biden and Trump posts. The network was divided into 37 distinct clusters, indicating multiple smaller communities within the larger Biden and Trump-focused groups.\nThe hypothesis that users with higher scores (more upvotes or higher score) would also have more central positions in the network was not supported. The correlation between score and centrality was negligible.\nWhile score wasn’t a strong indicator of influence, degree centrality (number of connections) was used to identify the most connected users. These users may play important roles in shaping discussions within their respective communities.\nIn terms of the sentiment analysis it seems that most comments were neutral or negative. And, users with similar sentiments tended to interact with each other.\n\nThis Network Analysis research offers interesting insights into the social dynamics of a politically charged online community. It highlights the presence of distinct communities, the complex interplay between user engagement (score) and influence (centrality), and the opportunity for further exploration of the factors shaping political discourse on platforms like Reddit.\n\n\nReferences\nBail C. A. (2016). Combining natural language processing and network analysis to examine how advocacy organizations stimulate conversation on social media. Proceedings of the National Academy of Sciences of the United States of America, 113(42), 11823–11828. https://doi.org/10.1073/pnas.1607151113\nConover, M., Ratkiewicz, J., Francisco, M., Goncalves, B., Menczer, F., & Flammini, A. (2021). Political Polarization on Twitter. Proceedings of the International AAAI Conference on Web and Social Media, 5(1), 89-96. https://doi.org/10.1609/icwsm.v5i1.14126"
  },
  {
    "objectID": "posts/Post 4 - Final Project/index.html#network-analysis-on-subreddit-rpolitics",
    "href": "posts/Post 4 - Final Project/index.html#network-analysis-on-subreddit-rpolitics",
    "title": "Network Analysis on subreddit r/politics",
    "section": "Network Analysis on subreddit r/politics",
    "text": "Network Analysis on subreddit r/politics\n\nIntroduction\nSocial media, including Reddit, serves as a prominent platform for discourse and community engagement. The “Politics” subreddit (r/politics) is a hub for discussions on political matters, attracting users who share news articles and express opinions (Bail, 2016).\nIn recent years, Reddit discussions have intensified, especially regarding U.S. politics, notably around Joe Biden and Donald Trump. Analyzing user interactions in r/politics offers insight into digital political landscapes (Conover et al., 2013).\nThis research aims to explore Reddit users’ connections within r/politics, focusing on Biden and Trump discussions. We seek to uncover community formation patterns and influencers through social network analysis. Specifically, we’ll examine users’ connectivity, community emergence, and the relationship between post popularity and user influence.\nBy addressing these questions, we contribute to understanding political discourse on Reddit and social media’s role in shaping public opinion.\n\n\nResearch Question\n\nHow are Reddit users connected in the “Politics” subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\nAre there different communities (networks) for Biden and Trump?\nHow are these group of users connected based on the sentiment of their comments?\nIs there a relationship between Score (net value between upvotes and downvotes) for a post, and how the user is connected to other users?\n\n\n\nHypothesis\nThis research is mainly exploratory, I am not expecting something in particular, so I don’t have an specific hypothesis, except for the research question number 4.\nIn relation to the fourth research question I can expect that:\nH1: an user’s higher score should be highly related to a higher centrality in the network.\n\n\nData\n\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n\n\nData Wrangling\n\nI scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package) and saved the objects generated from the scrapping to csv files\nThis subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\ngetwd()\n\n[1] \"C:/Users/fbeta/OneDrive/Blue Cognition/Blog/R-Blog-BlueCognition/posts/Post 4 - Final Project\"\n\n# Read large CSV file using fread\npolitik1 &lt;- fread(\"politics_comments1.csv\")\npolitik2 &lt;- fread(\"politics_comments2.csv\")\npolitik3 &lt;- fread(\"politics_comments3.csv\")\n\n#Checking the structure of the data sets\nglimpse(politik1)\n\nRows: 983\nColumns: 8\n$ V1        &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  &lt;IDate&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     &lt;chr&gt; \"Supreme Court starts arguments as Biden administration defe…\n$ text      &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n\nglimpse(politik2)\n\nRows: 983\nColumns: 16\n$ V1                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                &lt;chr&gt; \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             &lt;int&gt; 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 &lt;chr&gt; \"Supreme Court starts arguments as Biden adminis…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               &lt;int&gt; 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              &lt;dbl&gt; 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           &lt;int&gt; 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              &lt;int&gt; 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n\nglimpse(politik3)\n\nRows: 95,879\nColumns: 11\n$ V1         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     &lt;chr&gt; \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       &lt;chr&gt; \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  &lt;int&gt; 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    &lt;int&gt; 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id &lt;chr&gt; \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n\n\nAs we can see the the information in object “politik1” is redundant with the information in “politik2” so I won’t use “politik1” at all. “Politik2” contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. “politik3” contain detailed comments on each post and the hierarchical sequence of comments to each post.\nFor the purpose of this research we will define the users or authors to posts and comments as the nodes, and edges are defined as comments made in the same post; it does mean that the network will be undirected as I will consider only authors commenting in the same post but I won’t capture the direction of the comment (B is commenting to A post).\nLet’s do some data wrangling first:\n\n# Cleaning and wrangling\n\npolitik_df &lt;- politik2 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df &lt;- as_tibble(politik_df)\npolitik_df$date &lt;- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 &lt;- politik3 %&gt;% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 &lt;- as_tibble(politik_df2)\npolitik_df2$date &lt;- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n\nLooking at the comments from user “Automoderator”, it is like a Reddit moderator bot reminding rules of the forum, so I’ll delete the rows belonging to AutoModerator”. Also there are few commments where the author was “deleted”.\n\npolitik_df2 &lt;- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 &lt;- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n\nLet’s see how many nodes (authors/users) we got in this data set:\n\n#first I created a count column\npolitik_df3 &lt;- politik_df3 %&gt;% mutate(countid = \"1\")\npolitik_df3$countid &lt;- as.numeric(politik_df3$countid)\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n\n[1] 31554\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I’ll select a sample of posts to analyze.\nI’ll select the top 1% posts with more comments.\n\n#first let's see the distribution of number of comments\npercentiles &lt;- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n\n\nLet’s subset the data set with the top 1% posts in terms of comments and let’s see how many posts we have.\n\nsubset_politik2 &lt;- subset(politik_df, comments &gt;= 1439 )\nglimpse(subset_politik2)\n\nRows: 10\nColumns: 14\n$ url                   &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                &lt;chr&gt; \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 &lt;chr&gt; \"Letitia James fires back after Donald Trump's b…\n$ text                  &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             &lt;chr&gt; \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               &lt;int&gt; 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              &lt;dbl&gt; 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           &lt;int&gt; 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              &lt;int&gt; 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n\nlength(unique(subset_politik2$author))\n\n[1] 10\n\n\nWe got a data set with 10 original posts and 10 authors, this is now a more “reasonable” data frame to analyze.\nNow I need to identify these post into the “politik_df3” data set which contain all the hierarchical comments network.\n\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 4,902\nColumns: 10\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     &lt;chr&gt; \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       &lt;date&gt; 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    &lt;int&gt; 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id &lt;chr&gt; \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\n\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n\n[1] 3869\n\n\nWe got 982 posts but still +3.8k nodes, it is still high number of nodes.\nI’ll need a different approach.\nI’ll select 2 specific posts with a “median” number of comments. One post will be about Trump and another about Biden, specifically I will filter posts by containing the word “Biden” and “Trump” in the title of the post.\n\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n\n#Filtering the titles that contain Trump\ntrump_df &lt;- politik_df %&gt;% filter(grepl(\"Trump\", title))\ntrump_df$candidate &lt;- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump &lt;- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n})\n\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n\n\nThe median comment for Trump’s post is 74 comments.Therefore I’ll select the post with 74 comments.\n\n#Trump\ntrump_post &lt;- subset(trump_df, comments == 74 )\n\nNow let’s select Biden’s post\n\nsuppressWarnings({\n#Filtering the titles that contain Biden\nbiden_df &lt;- politik_df %&gt;% filter(grepl(\"Biden\", title))\nbiden_df$candidate &lt;- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden &lt;- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n\n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n\n\nMedian is 66.5 comments, let’s use the post with 67 comments.\n\n#Biden\nbiden_post &lt;- subset(biden_df, comments == 67 )\n\nNow I got the 2 main posts, let’s explore a bit those 2 posts.\n\n#merging the previous df's\ntrump_biden_df &lt;- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 &lt;- politik_df3 %&gt;%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 &lt;- subset_politik3 %&gt;%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n\nRows: 119\nColumns: 11\n$ url        &lt;chr&gt; \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     &lt;chr&gt; \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       &lt;date&gt; 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    &lt;int&gt; 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    &lt;chr&gt; \"It's very possible that both Biden and Trump are losing so…\n$ comment_id &lt;chr&gt; \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  &lt;chr&gt; \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n\n# Let's keep only the relevant columns\npolitik_final &lt;- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 &lt;- politik_final %&gt;%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level &gt; 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n\n[1] 80\n\nlength(unique(politik_final2$url))\n\n[1] 2\n\n\nNow we got 80 nodes (authors) from the 2 posts.\n\n\nAnalysis\nBefore proceeding with the network analysis, let’s explore a bit about the authors (users).\n\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_final2 &lt;- politik_final2 %&gt;% mutate(countid = \"1\")\npolitik_final2$countid &lt;- as.numeric(politik_final2$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 &lt;- data.table(politik_final2)\n\n#total posts grouped by author\ncount_table2 &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarise(Total_posts = sum(countid))\ncount_table2 &lt;- count_table2 %&gt;% arrange(desc(Total_posts))\nprint(count_table2)\n\n# A tibble: 80 × 2\n   author            Total_posts\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 BrtFrkwr                    7\n 2 betterwoke                  5\n 3 Hattopia                    4\n 4 TheBodyPolitic1             4\n 5 TheRandomInteger            4\n 6 AngusMcTibbins              3\n 7 Due-Shirt616                3\n 8 NoDesinformatziya           3\n 9 TheReddestOrange            3\n10 DarkwingDuckHunt            2\n# ℹ 70 more rows\n\nsummary_votes &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Total_Score = sum(score))\nsummary_votes &lt;- summary_votes %&gt;% arrange(desc(Total_Score))\nprint(summary_votes)\n\n# A tibble: 80 × 2\n   author             Total_Score\n   &lt;chr&gt;                    &lt;int&gt;\n 1 r-m-russell                 76\n 2 AngusMcTibbins              68\n 3 OverlyComplexPants          40\n 4 BrtFrkwr                    33\n 5 betterwoke                  28\n 6 TheBodyPolitic1             24\n 7 penis_berry_crunch          22\n 8 Due-Shirt616                21\n 9 NoDesinformatziya           20\n10 YeaterdaysQuim              20\n# ℹ 70 more rows\n\n#Score as a proportion of comments\nsummary_score_ratio &lt;- politik_table2 %&gt;% group_by(author) %&gt;% summarize(Ratio_score_per_comment = sum(score)/sum(countid))\nsummary_score_ratio &lt;- summary_score_ratio %&gt;% arrange(desc(Ratio_score_per_comment))\nprint(summary_score_ratio)\n\n# A tibble: 80 × 2\n   author             Ratio_score_per_comment\n   &lt;chr&gt;                                &lt;dbl&gt;\n 1 r-m-russell                           76  \n 2 OverlyComplexPants                    40  \n 3 AngusMcTibbins                        22.7\n 4 penis_berry_crunch                    22  \n 5 YeaterdaysQuim                        20  \n 6 fxkatt                                20  \n 7 grixorbatz                            17  \n 8 Sea_Engine4333                        16  \n 9 Quiet_Dimensions                      14  \n10 hdiggyh                               14  \n# ℹ 70 more rows\n\n\nI would expect that nodes (users) with higher score and/or higher score per comment should be predominant (central) in the network.\nFor instance we got the following users in the top 5 in terms of score: “r-m-russell”, “AngusMcTibbins”, “OverlyComplexPants”, “BrtFrkwr” and “betterwoke”.\nOn the other hand here the top 5 users with highest score per comment: “r-m-russell”, “OverlyComplexPants”, “AngusMcTibbins”, “penis_berry_crunch”, and “YeaterdaysQuim”.\nNow I am ready to work on this data set for the Network analysis.\n\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 &lt;- politik_final2 %&gt;%\n  rename(distance = Level)\n\n#identify who is commenting on the same post\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level &lt;- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 &lt;- politik_final2 %&gt;%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 &lt;- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 &lt;- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes &lt;- select(politik_final3, c(\"score\", \"candidate\", \"distance\"))\n\nI’ll prepare the adjacency matrix:\n\npolitik_m &lt;- select(politik_final3, c(\"comment_id2\", \"author\"))\n\n# Identify unique names and codes\nunique_names &lt;- unique(politik_final3$author)\nunique_codes &lt;- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix &lt;- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code &lt;- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) &gt; 0) {\n      adj_matrix[unique_names[i], unique_names[j]] &lt;- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) &lt;- 0\n\n\nNetwork Analysis\nNow let’s explore the Network.\nIt is important to mention that in this research we will assume an undirected network. We are only considering comments within the same post, not specific “direction” of each comment among users.\n\n#load packages\nsuppressPackageStartupMessages(library(network))\nlibrary(sna)\nlibrary(statnet)\n\npolitik.n &lt;- network(adj_matrix, directed = FALSE)\npolitik.n\n\n Network attributes:\n  vertices = 80 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 158 \n    missing edges= 0 \n    non-missing edges= 158 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\nWe got 80 nodes and 316 edges.\nLet’s explore the network. I’ll calculate the census for Dyads and triads:\n\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n\n     Mut Asym Null\n[1,] 158    0 3002\n\nsum(sna::triad.census(politik.n))\n\n[1] 82160\n\nsna::triad.census(politik.n)\n\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n\n\nIn terms of Dyads, we got 158 mutual connections and 3002 null connections.\nIn terms of Triads, we got 82160 triads in total: about 70k null triads, 11k open triads (one connection exist), 179 where 2 connections exist, and 337 closed triads.\nIt seems a disperse network, but let’s see Transitivity and Density:\nLet’s check the Transitivity coefficient\n\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n\n[1] 0.8495798\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\nLet’s see the density:\n\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n\n[1] 0.05\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\nLet’s visualize the network\n\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nWithout isolated nodes and labels\n\n# Plot the network\nplot(politik.n, displaylabels = F, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n\n\n\n\n\n\n\n\nLet’s now include the Candidate as attribute. We will clasify the users as per their comments to Biden or Trump posts.\n\n#I'll create a column with Biden true-false attribute\npolitik_final3at &lt;- politik_final3 %&gt;%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how authonrs in Biden and Trump are interacting\nnodeColors&lt;-ifelse(politik_final3at$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=T, main = \"Authors Network by Candidate\") #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nLet’s exclude isolated nodes\n\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=F, main = \"Authors Network by Candidate (excluding isolated nodes)\") #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n\n\n\n\n\n\n\n\nIt looks like there are closed communities not connected among them. Which is consistent with the Transitivity vs Density finding before.\nIn particular Biden’s commentors tend to be together and Trump’s commentors seems more disperse.\nLet’s see who are the authors with highest degree centrality\n\n# create a dataset of vertex names and degree: statnet\npolitik.nodes.df &lt;- data.frame(name = politik.n %v% \"vertex.names\",\n                            degree = sna::degree(politik.n))\n\npolitik_table7 &lt;- data.table(politik.nodes.df)\n\n#order by centrality degree\npolitik_table7 %&gt;% arrange(desc(degree)) %&gt;%\nslice(1:10)\n\n                  name degree\n                &lt;char&gt;  &lt;num&gt;\n 1:           Knoxcore     30\n 2:  NoDesinformatziya     30\n 3:           BrtFrkwr     28\n 4:   TheRandomInteger     26\n 5:    TheBodyPolitic1     20\n 6:         Fasefirst2     20\n 7:           cdiddy19     20\n 8: Invincible_auxcord     20\n 9:     Bulky-You-5657     20\n10:        nickmiele22     20\n\nsummary(politik_table7)\n\n     name               degree    \n Length:80          Min.   : 0.0  \n Class :character   1st Qu.: 0.0  \n Mode  :character   Median : 6.0  \n                    Mean   : 7.9  \n                    3rd Qu.:14.5  \n                    Max.   :30.0  \n\n\nLet’s explore the correlation between centrality degree and score (remember it is the net value between upvotes and downvotes).\nI am expecting a significant positive relationship between soore and degree centrality.\n\nsuppressPackageStartupMessages(library(igraph))\n\n# Create the igraph object\npolitik.ig &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")  # Undirected by default\n\n# Calculate degree centrality for each node\ndegree_centrality &lt;- degree(politik.ig, mode = \"all\")\n\n# If nodes do not have names, you can use node IDs\nif (is.null(V(politik.ig)$name)) {\n  V(politik.ig)$name &lt;- as.character(1:vcount(politik.ig))\n}\n\n# Check node names\nnode_names &lt;- V(politik.ig)$name\n\n# Create a sample data frame with some values for each node\n# Ensure the data frame has the same node identifiers as the graph\ndf &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  value = runif(vcount(politik.ig), 1, 100)  # Random values between 1 and 100\n)\n\n# Convert degree centrality to a data frame\ndegree_centrality_df &lt;- data.frame(\n  author = node_names,  # Node names or IDs\n  degree_centrality = degree_centrality  # Degree centrality values\n)\n\n# Merge the degree centrality data frame with the existing data frame\nmerged_df &lt;- merge(df, degree_centrality_df, by = \"author\", all = TRUE)\nmerged_df2 &lt;- merge(merged_df, politik_final2, by = \"author\", all = TRUE)\ndegree_scoredf &lt;- select(merged_df2, c(\"degree_centrality\", \"score\"))\n\n# Calculate the correlation coefficient between 'score' and 'degree_centrality'\ncor_matrix1 &lt;- cor(degree_scoredf, use = \"complete.obs\")\ncor_matrix1\n\n                  degree_centrality       score\ndegree_centrality       1.000000000 0.001921923\nscore                   0.001921923 1.000000000\n\n\nSeems that the relationship between score and degree centrality is very low (0.002).\nSo I can’t accept my hypothesis.\nI am curious about the top 5 users with higher degree centrality.\n\ndegree_scoredf3 &lt;- select(merged_df2, c(\"degree_centrality\", \"author\"))\nsubset_df &lt;- distinct(degree_scoredf3, author, .keep_all = TRUE)\n\nsubset_df %&gt;% arrange(desc(degree_centrality))\n\n   degree_centrality               author\n1                 15             Knoxcore\n2                 15    NoDesinformatziya\n3                 14             BrtFrkwr\n4                 13     TheRandomInteger\n5                 10       Bulky-You-5657\n6                 10             cdiddy19\n7                 10  Exciting_Slice_9492\n8                 10           Fasefirst2\n9                 10        hilljack26301\n10                10   Invincible_auxcord\n11                10             Kamelasa\n12                10          nickmiele22\n13                10      TheBodyPolitic1\n14                 8   Admirable_Bad_5649\n15                 8           betterwoke\n16                 8     DarkwingDuckHunt\n17                 8          Scarlettail\n18                 8        Spara-Extreme\n19                 8     TheReddestOrange\n20                 8     Thick-Return1694\n21                 7          bakeacake45\n22                 6            AliMcGraw\n23                 6       AngusMcTibbins\n24                 6             Hattopia\n25                 6    hellocattlecookie\n26                 6   Mundane_Rabbit7751\n27                 6   OverlyComplexPants\n28                 6         Roasted_Butt\n29                 6    SeegsonSynthetics\n30                 6             Shaunair\n31                 4             caserock\n32                 4     CecilTWashington\n33                 4            gefjunhel\n34                 4   penis_berry_crunch\n35                 4          r-m-russell\n36                 3         Due-Shirt616\n37                 3              hdiggyh\n38                 3             iuthnj34\n39                 3               JeffMo\n40                 3        PineTreeBanjo\n41                 3              Purify5\n42                 3     Quiet_Dimensions\n43                 3       YeaterdaysQuim\n44                 1  Candid_Chicken_9246\n45                 1   FijiFanBotNotGay69\n46                 1        Happypappy213\n47                 1            hindusoul\n48                 1      InGreedWeTrust3\n49                 1              LariRed\n50                 1   physical_graffitti\n51                 1            Tower6011\n52                 0              bck1999\n53                 0     bloombergopinion\n54                 0          cryolongman\n55                 0          Donut131313\n56                 0              eldred2\n57                 0      Empty-Rise-4409\n58                 0     fore_skin_walker\n59                 0               fxkatt\n60                 0           grixorbatz\n61                 0            Guttenber\n62                 0        HonoredPeople\n63                 0     ImportantNeck491\n64                 0        inconsistent3\n65                 0       JubalHarshaw23\n66                 0             njman100\n67                 0          No_Yak_6227\n68                 0       Odd_Tiger_2278\n69                 0 platanthera_ciliaris\n70                 0        PoopieButt317\n71                 0  Practical_Shop_4055\n72                 0         RUIN_NATION_\n73                 0       Sea_Engine4333\n74                 0          SeaSuch2077\n75                 0             spotspam\n76                 0          stjoechief1\n77                 0          StormOk7544\n78                 0              syg-123\n79                 0               th1961\n80                 0         Willowgirl78\n\n\nUsers with higher degree centrality are “Knoxcore”, “NoDesinformatziya”, “BrtFrkwr”, “TheRandomInteger”, and “Bulky-You-5657”\nOut of these 5 nodes only one of them (“BrtFrkwr”) is also in the top 5 related to score. So it is consistent with the low correlation between score and centrality.\nLet’s now check what clusters (communities) we do have in this network.\n\n# run clustering algorithm: fast_greedy\npolitik.fg &lt;- igraph::cluster_fast_greedy(politik.ig)\n# inspect clustering object\npolitik.fg\n\nIGRAPH clustering fast greedy, groups: 37, mod: 0.63\n+ groups:\n  $`1`\n   [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n   [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n   [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n  [10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n  [13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n  [16] \"Kamelasa\"           \n  \n  $`2`\n   [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n  + ... omitted several groups/vertices\n\nigraph::groups(politik.fg)\n\n$`1`\n [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n[10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n[13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n[16] \"Kamelasa\"           \n\n$`2`\n [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n [4] \"Roasted_Butt\"        \"BrtFrkwr\"            \"TheRandomInteger\"   \n [7] \"Shaunair\"            \"betterwoke\"          \"DarkwingDuckHunt\"   \n[10] \"Thick-Return1694\"    \"Spara-Extreme\"       \"Admirable_Bad_5649\" \n[13] \"TheReddestOrange\"    \"Scarlettail\"         \"Tower6011\"          \n[16] \"Candid_Chicken_9246\"\n\n$`3`\n[1] \"r-m-russell\"        \"penis_berry_crunch\" \"CecilTWashington\"  \n[4] \"caserock\"           \"gefjunhel\"         \n\n$`4`\n[1] \"iuthnj34\"       \"YeaterdaysQuim\" \"hdiggyh\"        \"Purify5\"       \n\n$`5`\n[1] \"Quiet_Dimensions\" \"Due-Shirt616\"     \"JeffMo\"           \"PineTreeBanjo\"   \n\n$`6`\n[1] \"physical_graffitti\" \"hindusoul\"         \n\n$`7`\n[1] \"LariRed\"         \"InGreedWeTrust3\"\n\n$`8`\n[1] \"Happypappy213\"      \"FijiFanBotNotGay69\"\n\n$`9`\n[1] \"fxkatt\"\n\n$`10`\n[1] \"Sea_Engine4333\"\n\n$`11`\n[1] \"Practical_Shop_4055\"\n\n$`12`\n[1] \"PoopieButt317\"\n\n$`13`\n[1] \"inconsistent3\"\n\n$`14`\n[1] \"Empty-Rise-4409\"\n\n$`15`\n[1] \"stjoechief1\"\n\n$`16`\n[1] \"th1961\"\n\n$`17`\n[1] \"platanthera_ciliaris\"\n\n$`18`\n[1] \"No_Yak_6227\"\n\n$`19`\n[1] \"fore_skin_walker\"\n\n$`20`\n[1] \"HonoredPeople\"\n\n$`21`\n[1] \"bloombergopinion\"\n\n$`22`\n[1] \"RUIN_NATION_\"\n\n$`23`\n[1] \"ImportantNeck491\"\n\n$`24`\n[1] \"grixorbatz\"\n\n$`25`\n[1] \"spotspam\"\n\n$`26`\n[1] \"JubalHarshaw23\"\n\n$`27`\n[1] \"StormOk7544\"\n\n$`28`\n[1] \"njman100\"\n\n$`29`\n[1] \"Odd_Tiger_2278\"\n\n$`30`\n[1] \"cryolongman\"\n\n$`31`\n[1] \"Willowgirl78\"\n\n$`32`\n[1] \"Guttenber\"\n\n$`33`\n[1] \"SeaSuch2077\"\n\n$`34`\n[1] \"syg-123\"\n\n$`35`\n[1] \"bck1999\"\n\n$`36`\n[1] \"Donut131313\"\n\n$`37`\n[1] \"eldred2\"\n\n\nThere are 2 main clusters in the network.\nLet’s see the density of each cluster using block model function.\n\nprint(blockmodel(politik.n, politik.fg$membership)$block.model,\n      digits = 2)\n\n         Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8\nBlock 1     0.62    0.00       0       0       0       0       0       0\nBlock 2     0.00    0.48       0       0       0       0       0       0\nBlock 3     0.00    0.00       1       0       0       0       0       0\nBlock 4     0.00    0.00       0       1       0       0       0       0\nBlock 5     0.00    0.00       0       0       1       0       0       0\nBlock 6     0.00    0.00       0       0       0       1       0       0\nBlock 7     0.00    0.00       0       0       0       0       1       0\nBlock 8     0.00    0.00       0       0       0       0       0       1\nBlock 9     0.00    0.00       0       0       0       0       0       0\nBlock 10    0.00    0.00       0       0       0       0       0       0\nBlock 11    0.00    0.00       0       0       0       0       0       0\nBlock 12    0.00    0.00       0       0       0       0       0       0\nBlock 13    0.00    0.00       0       0       0       0       0       0\nBlock 14    0.00    0.00       0       0       0       0       0       0\nBlock 15    0.00    0.00       0       0       0       0       0       0\nBlock 16    0.00    0.00       0       0       0       0       0       0\nBlock 17    0.00    0.00       0       0       0       0       0       0\nBlock 18    0.00    0.00       0       0       0       0       0       0\nBlock 19    0.00    0.00       0       0       0       0       0       0\nBlock 20    0.00    0.00       0       0       0       0       0       0\nBlock 21    0.00    0.00       0       0       0       0       0       0\nBlock 22    0.00    0.00       0       0       0       0       0       0\nBlock 23    0.00    0.00       0       0       0       0       0       0\nBlock 24    0.00    0.00       0       0       0       0       0       0\nBlock 25    0.00    0.00       0       0       0       0       0       0\nBlock 26    0.00    0.00       0       0       0       0       0       0\nBlock 27    0.00    0.00       0       0       0       0       0       0\nBlock 28    0.00    0.00       0       0       0       0       0       0\nBlock 29    0.00    0.00       0       0       0       0       0       0\nBlock 30    0.00    0.00       0       0       0       0       0       0\nBlock 31    0.00    0.00       0       0       0       0       0       0\nBlock 32    0.00    0.00       0       0       0       0       0       0\nBlock 33    0.00    0.00       0       0       0       0       0       0\nBlock 34    0.00    0.00       0       0       0       0       0       0\nBlock 35    0.00    0.00       0       0       0       0       0       0\nBlock 36    0.00    0.00       0       0       0       0       0       0\nBlock 37    0.00    0.00       0       0       0       0       0       0\n         Block 9 Block 10 Block 11 Block 12 Block 13 Block 14 Block 15 Block 16\nBlock 1        0        0        0        0        0        0        0        0\nBlock 2        0        0        0        0        0        0        0        0\nBlock 3        0        0        0        0        0        0        0        0\nBlock 4        0        0        0        0        0        0        0        0\nBlock 5        0        0        0        0        0        0        0        0\nBlock 6        0        0        0        0        0        0        0        0\nBlock 7        0        0        0        0        0        0        0        0\nBlock 8        0        0        0        0        0        0        0        0\nBlock 9      NaN        0        0        0        0        0        0        0\nBlock 10       0      NaN        0        0        0        0        0        0\nBlock 11       0        0      NaN        0        0        0        0        0\nBlock 12       0        0        0      NaN        0        0        0        0\nBlock 13       0        0        0        0      NaN        0        0        0\nBlock 14       0        0        0        0        0      NaN        0        0\nBlock 15       0        0        0        0        0        0      NaN        0\nBlock 16       0        0        0        0        0        0        0      NaN\nBlock 17       0        0        0        0        0        0        0        0\nBlock 18       0        0        0        0        0        0        0        0\nBlock 19       0        0        0        0        0        0        0        0\nBlock 20       0        0        0        0        0        0        0        0\nBlock 21       0        0        0        0        0        0        0        0\nBlock 22       0        0        0        0        0        0        0        0\nBlock 23       0        0        0        0        0        0        0        0\nBlock 24       0        0        0        0        0        0        0        0\nBlock 25       0        0        0        0        0        0        0        0\nBlock 26       0        0        0        0        0        0        0        0\nBlock 27       0        0        0        0        0        0        0        0\nBlock 28       0        0        0        0        0        0        0        0\nBlock 29       0        0        0        0        0        0        0        0\nBlock 30       0        0        0        0        0        0        0        0\nBlock 31       0        0        0        0        0        0        0        0\nBlock 32       0        0        0        0        0        0        0        0\nBlock 33       0        0        0        0        0        0        0        0\nBlock 34       0        0        0        0        0        0        0        0\nBlock 35       0        0        0        0        0        0        0        0\nBlock 36       0        0        0        0        0        0        0        0\nBlock 37       0        0        0        0        0        0        0        0\n         Block 17 Block 18 Block 19 Block 20 Block 21 Block 22 Block 23\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17      NaN        0        0        0        0        0        0\nBlock 18        0      NaN        0        0        0        0        0\nBlock 19        0        0      NaN        0        0        0        0\nBlock 20        0        0        0      NaN        0        0        0\nBlock 21        0        0        0        0      NaN        0        0\nBlock 22        0        0        0        0        0      NaN        0\nBlock 23        0        0        0        0        0        0      NaN\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 24 Block 25 Block 26 Block 27 Block 28 Block 29 Block 30\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24      NaN        0        0        0        0        0        0\nBlock 25        0      NaN        0        0        0        0        0\nBlock 26        0        0      NaN        0        0        0        0\nBlock 27        0        0        0      NaN        0        0        0\nBlock 28        0        0        0        0      NaN        0        0\nBlock 29        0        0        0        0        0      NaN        0\nBlock 30        0        0        0        0        0        0      NaN\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 31 Block 32 Block 33 Block 34 Block 35 Block 36 Block 37\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31      NaN        0        0        0        0        0        0\nBlock 32        0      NaN        0        0        0        0        0\nBlock 33        0        0      NaN        0        0        0        0\nBlock 34        0        0        0      NaN        0        0        0\nBlock 35        0        0        0        0      NaN        0        0\nBlock 36        0        0        0        0        0      NaN        0\nBlock 37        0        0        0        0        0        0      NaN\n\n\nThe blocks 1 and 2 in our network seems dense.\n\ndf_comm &lt;- data.frame(\n  Node = V(politik.ig)$name,\n  Community = politik.fg$membership,\n  Degree = degree_centrality\n)\n\n# 4. Find maximum degree for each community\n\nhighest_degree_nodes &lt;- df_comm %&gt;%\n  group_by(Community) %&gt;%\n  filter(Degree == max(Degree)) %&gt;%\n  ungroup()\n\nhighest_degree_nodes %&gt;% arrange(-desc(Community))\n\n# A tibble: 51 × 3\n   Node               Community Degree\n   &lt;chr&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Knoxcore                   1     15\n 2 NoDesinformatziya          1     15\n 3 BrtFrkwr                   2     14\n 4 r-m-russell                3      4\n 5 penis_berry_crunch         3      4\n 6 CecilTWashington           3      4\n 7 caserock                   3      4\n 8 gefjunhel                  3      4\n 9 iuthnj34                   4      3\n10 YeaterdaysQuim             4      3\n# ℹ 41 more rows\n\n\nLet’s now visualize the communities including the nodes with highest degree centrality for the main communities (1 and 2):\n\n# Identify nodes with high degree centrality (e.g., top 3)\ntop_nodes &lt;- names(sort(degree_centrality, decreasing = TRUE))[1:5]\n\n# Create a custom labeling vector\nnode_labels &lt;- rep(NA, vcount(politik.ig))\n# Set the labels for the nodes with high degree centrality\nnode_labels[top_nodes] &lt;- top_nodes\n\n# Identify isolated nodes\nisolated_nodes &lt;- which(degree(politik.ig) == 0)\n\n# Remove isolated nodes from the network\nnetwork_no_isolated &lt;- delete_vertices(politik.ig, isolated_nodes)\n\nplot(network_no_isolated,\n     vertex.label = node_labels,  \n     vertex.label.cex = 0.8,\n     vertex.color = membership(politik.fg),\n     vertex.label.color = \"black\",\n     vertex.shape = \"sphere\",\n     layout = layout_with_fr,\n     main = \"Communities without isolated nodes and labeling top 5 central nodes\")\n\n\n\n\n\n\n\n\nIt looks like while we have 2 main communities the nodes with highest degree centrality are mostly in one of the communities.\nIt seems like that group of authors are close among them and at the same time are central in the network.\n\n\nSentiment Analysis and the Network\nLastly let’s add a new attribute to the network using text analysis, in particular sentiment analysis.\nLet’s get the sentiment for each author’s comments.\n\nlibrary(sentimentr)\n\n#labeling based on the sentiment score\ncomments &lt;- politik_final3$comment\nget_sentiment_label &lt;- function(ave_sentiment) {\n  if (ave_sentiment &gt; 0.1) {\n    return(\"Positive\")\n  } else if (ave_sentiment &lt; -0.1) {\n    return(\"Negative\")\n  } else {\n    return(\"Neutral\")\n  }\n}\nsentiment_scores &lt;- sentiment_by(x = comments, text.var = comments)\n\n#adding the label for each author in the data set\npolitik_final3$sentiment &lt;- sapply(sentiment_scores$ave_sentiment, get_sentiment_label)\n\nNow let’s visualize the network by sentiment of each node.\n\n# Create a graph object from the data frame\ng &lt;- graph_from_data_frame(politik.n, directed = FALSE)\n\n# Add node attributes to the graph\nV(g)$sentiment &lt;- politik_final3$sentiment[match(V(g)$name, politik_final3$author)]\n\n\n# Define color palette for categories\ncolor_palette &lt;- c(\"Negative\" = \"red\", \"Neutral\" = \"blue\", \"Positive\" = \"lightgreen\")\n\n# Visualize the network with colored nodes\nplot(g, vertex.color = color_palette[V(g)$sentiment], layout = layout_nicely, vertex.label = NA, vertex.size = 7, isolates=TRUE, main = \"Authors Network by Sentiment\")\nlegend(\"bottomright\", legend = c(\"Positive\", \"Neutral\", \"Negative\"), col = c(\"lightgreen\", \"blue\", \"red\"), pch = c(21, 21), title = \"Node Sentiment\")\n\n\n\n\n\n\n\n\nFrom this chart it looks like:\n\nMost of the sentiments are either neutral or negative\nSentiments tend to get closer, or group among them.\n\n\n\n\n\nConclusion\n\nThe network was found to be sparse (low density) but highly interconnected within subgroups (high transitivity), suggesting the presence of distinct communities.\nThe visualization of the network confirmed the presence of communities, showing separate clusters of users commenting on Biden and Trump posts. The network was divided into 37 distinct clusters, indicating multiple smaller communities within the larger Biden and Trump-focused groups.\nThe hypothesis that users with higher scores (more upvotes or higher score) would also have more central positions in the network was not supported. The correlation between score and centrality was negligible.\nWhile score wasn’t a strong indicator of influence, degree centrality (number of connections) was used to identify the most connected users. These users may play important roles in shaping discussions within their respective communities.\nIn terms of the sentiment analysis it seems that most comments were neutral or negative. And, users with similar sentiments tended to interact with each other.\n\nThis Network Analysis research offers interesting insights into the social dynamics of a politically charged online community. It highlights the presence of distinct communities, the complex interplay between user engagement (score) and influence (centrality), and the opportunity for further exploration of the factors shaping political discourse on platforms like Reddit.\n\n\nReferences\nBail C. A. (2016). Combining natural language processing and network analysis to examine how advocacy organizations stimulate conversation on social media. Proceedings of the National Academy of Sciences of the United States of America, 113(42), 11823–11828. https://doi.org/10.1073/pnas.1607151113\nConover, M., Ratkiewicz, J., Francisco, M., Goncalves, B., Menczer, F., & Flammini, A. (2021). Political Polarization on Twitter. Proceedings of the International AAAI Conference on Web and Social Media, 5(1), 89-96. https://doi.org/10.1609/icwsm.v5i1.14126"
  }
]