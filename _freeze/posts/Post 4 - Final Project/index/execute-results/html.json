{
  "hash": "20a674fe33ca88684b9de567b8711973",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Network Analysis on subreddit r/politics\"\nsubtitle: \"Social Network Analysis\"\nauthor: \"Felix Betancourt\"\ndate: \"May 16, 2024\"\nformat: \n  html:\n    toc: true\n    toc-depth: 2\n    toc-title: Contents\n    toc-location: left\n    code-fold: false\n    html-math-method: katex\n    theme: flatly\n    smooth-scroll: true\n    link-external-icon: true\n    link-external-newwindow: true\n    citations-hover: true\n    footnotes-hover: true\n    font-size: 80%\neditor: visual\nimage: \"net_image_post.jpg\"\n---\n\n\n![](net_image_post.jpg)\n\n## Network Analysis on subreddit r/politics\n\n### Introduction\n\nSocial media, including Reddit, serves as a prominent platform for discourse and community engagement. The \"Politics\" subreddit (r/politics) is a hub for discussions on political matters, attracting users who share news articles and express opinions (Bail, 2016).\n\nIn recent years, Reddit discussions have intensified, especially regarding U.S. politics, notably around Joe Biden and Donald Trump. Analyzing user interactions in r/politics offers insight into digital political landscapes (Conover et al., 2013).\n\nThis research aims to explore Reddit users' connections within r/politics, focusing on Biden and Trump discussions. We seek to uncover community formation patterns and influencers through social network analysis. Specifically, we'll examine users' connectivity, community emergence, and the relationship between post popularity and user influence.\n\nBy addressing these questions, we contribute to understanding political discourse on Reddit and social media's role in shaping public opinion.\n\n### Research Question\n\n1.  How are Reddit users connected in the \"Politics\" subreddit (r/politics), particularly when it comes to topics related to Biden and Trump?\n\n2.  Are there different communities (networks) for Biden and Trump?\n\n3.  How are these group of users connected based on the sentiment of their comments?\n\n4.  Is there a relationship between Score (net value between upvotes and downvotes) for a post, and how the user is connected to other users?\n\n### Hypothesis\n\nThis research is mainly exploratory, I am not expecting something in particular, so I don't have an specific hypothesis, except for the research question number 4.\n\nIn relation to the fourth research question I can expect that:\n\nH1: an user's higher score should be highly related to a higher centrality in the network.\n\n### Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressWarnings({\nsuppressPackageStartupMessages(library(tidytext))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(quanteda))\nsuppressPackageStartupMessages(library(quanteda.textplots))\nsuppressPackageStartupMessages(library(janitor))\nsuppressPackageStartupMessages(library(RCurl))\nsuppressPackageStartupMessages(library(data.table))\n})\n```\n:::\n\n\n#### Data Wrangling\n\n1.  I scraped data from the Politics subreddit (r/politics) on April 2nd 2024 using R (RedditExtractoR package) and saved the objects generated from the scrapping to csv files\n\n2.  This subreddit has 8.5 million users, so the data can be very extensive, however the package used here (RedditExtractoR) pulled the last 1000 post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetwd()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"C:/Users/fbeta/OneDrive/Blue Cognition/Blog/R-Blog-BlueCognition/posts/Post 4 - Final Project\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Read large CSV file using fread\npolitik1 <- fread(\"politics_comments1.csv\")\npolitik2 <- fread(\"politics_comments2.csv\")\npolitik3 <- fread(\"politics_comments3.csv\")\n\n#Checking the structure of the data sets\nglimpse(politik1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 983\nColumns: 8\n$ V1        <chr> \"Supreme Court starts arguments as Biden administration defe…\n$ date_utc  <IDate> 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ timestamp <int> 1711463500, 1711463310, 1711462666, 1711462651, 1711462570, …\n$ title     <chr> \"Supreme Court starts arguments as Biden administration defe…\n$ text      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Oral argument i…\n$ subreddit <chr> \"politics\", \"politics\", \"politics\", \"politics\", \"politics\", …\n$ comments  <int> 34, 21, 194, 24, 43, 15, 150, 251, 18, 26, 10, 597, 27, 299,…\n$ url       <chr> \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme_…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(politik2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 983\nColumns: 16\n$ V1                    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ url                   <chr> \"https://www.reddit.com/r/politics/comments/1bo9…\n$ author                <chr> \"Cybertronian1512\", \"ban_hus\", \"coasterghost\", \"…\n$ date                  <chr> \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/202…\n$ timestamp             <int> 1711463500, 1711463310, 1711462666, 1711462651, …\n$ title                 <chr> \"Supreme Court starts arguments as Biden adminis…\n$ text                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"Ora…\n$ subreddit             <chr> \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 <int> 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ upvotes               <int> 304, 127, 1250, 791, 421, 226, 4033, 4842, 197, …\n$ downvotes             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ up_ratio              <dbl> 0.95, 0.92, 0.95, 0.96, 0.91, 0.91, 0.97, 0.97, …\n$ total_awards_received <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ golds                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ cross_posts           <int> 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, …\n$ comments              <int> 34, 21, 194, 24, 43, 15, 150, 250, 18, 26, 10, 5…\n```\n\n\n:::\n\n```{.r .cell-code}\nglimpse(politik3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 95,879\nColumns: 11\n$ V1         <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ url        <chr> \"https://www.reddit.com/r/politics/comments/1bo9ce0/supreme…\n$ author     <chr> \"AutoModerator\", \"EmmaLouLove\", \"ctguy54\", \"EmmaLouLove\", \"…\n$ date       <chr> \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2024\", \"3/26/2…\n$ timestamp  <int> 1711463501, 1711465125, 1711466287, 1711466455, 1711467091,…\n$ score      <int> 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ upvotes    <int> 1, 79, 41, 13, 19, 13, 7, 6, 3, 4, 1, 4, 2, 5, 5, 1, 1, 28,…\n$ downvotes  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"\\r\\nAs a reminder, this subreddit [is for civil discussion…\n$ comment_id <chr> \"1\", \"2\", \"2_1\", \"2_1_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_…\n```\n\n\n:::\n:::\n\n\nAs we can see the the information in object \"politik1\" is redundant with the information in \"politik2\" so I won't use \"politik1\" at all. \"Politik2\" contain information about the title of the post, author, and some numeric information like up/down votes, number of replies to the post. \"politik3\" contain detailed comments on each post and the hierarchical sequence of comments to each post.\n\nFor the purpose of this research we will define the users or authors to posts and comments as the nodes, and edges are defined as comments made in the same post; it does mean that the network will be undirected as I will consider only authors commenting in the same post but I won't capture the direction of the comment (B is commenting to A post).\n\nLet's do some data wrangling first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cleaning and wrangling\n\npolitik_df <- politik2 %>% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df <- as_tibble(politik_df)\npolitik_df$date <- as.Date(politik_df$date, format = \"%m/%d/%Y\")\n\n\npolitik_df2 <- politik3 %>% select(-V1, -timestamp) #eliminating non-relevant columns\npolitik_df2 <- as_tibble(politik_df2)\npolitik_df2$date <- as.Date(politik_df2$date, format = \"%m/%d/%Y\")\n```\n:::\n\n\nLooking at the comments from user \"Automoderator\", it is like a Reddit moderator bot reminding rules of the forum, so I'll delete the rows belonging to AutoModerator\". Also there are few commments where the author was \"deleted\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitik_df2 <- politik_df2[-(which(politik_df2$author %in% \"AutoModerator\")),]\npolitik_df3 <- politik_df2[-(which(politik_df2$author %in% \"[deleted]\")),]\n```\n:::\n\n\nLet's see how many nodes (authors/users) we got in this data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#first I created a count column\npolitik_df3 <- politik_df3 %>% mutate(countid = \"1\")\npolitik_df3$countid <- as.numeric(politik_df3$countid)\n\n#How many authors (nodes) we have here?\nlength(unique(politik_df3$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 31554\n```\n\n\n:::\n:::\n\n\nIn the data set there are about +31k users/authors (nodes), which is way too much nodes for the purpose of my research, so I'll select a sample of posts to analyze.\n\nI'll select the top 1% posts with more comments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#first let's see the distribution of number of comments\npercentiles <- quantile(politik_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   25%    50%    75%    90%    95%    99% \n  20.0   46.0  115.0  338.6  576.2 1439.1 \n```\n\n\n:::\n:::\n\n\nLet's subset the data set with the top 1% posts in terms of comments and let's see how many posts we have.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubset_politik2 <- subset(politik_df, comments >= 1439 )\nglimpse(subset_politik2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 14\n$ url                   <chr> \"https://www.reddit.com/r/politics/comments/1bo5…\n$ author                <chr> \"newsweek\", \"thenewrepublic\", \"UWCG\", \"twenafees…\n$ date                  <date> 2024-03-26, 2024-03-26, 2024-03-27, 2024-03-27,…\n$ title                 <chr> \"Letitia James fires back after Donald Trump's b…\n$ text                  <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"\n$ subreddit             <chr> \"politics\", \"politics\", \"politics\", \"politics\", …\n$ score                 <int> 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ upvotes               <int> 12409, 28546, 32935, 25237, 13487, 14411, 17102,…\n$ downvotes             <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ up_ratio              <dbl> 0.92, 0.93, 0.91, 0.93, 0.91, 0.91, 0.91, 0.94, …\n$ total_awards_received <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ golds                 <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ cross_posts           <int> 1, 2, 5, 7, 9, 2, 3, 3, 6, 3\n$ comments              <int> 1476, 2018, 3564, 2419, 1523, 1677, 2291, 1668, …\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(unique(subset_politik2$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\nWe got a data set with 10 original posts and 10 authors, this is now a more \"reasonable\" data frame to analyze.\n\nNow I need to identify these post into the \"politik_df3\" data set which contain all the hierarchical comments network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubset_politik3 <- politik_df3 %>%\n         filter(url %in% subset_politik2$url)\n\n#let's see the df now \nglimpse(subset_politik3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,902\nColumns: 10\n$ url        <chr> \"https://www.reddit.com/r/politics/comments/1bo5tnj/letitia…\n$ author     <chr> \"OokLeeNooma\", \"AusToddles\", \"dancode\", \"GrafZeppelin127\", …\n$ date       <date> 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26, 2024-03-26…\n$ score      <int> 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ upvotes    <int> 5480, 2444, 1054, 871, 792, 414, 212, 31, 2, 3, 8, 26, 3, 3…\n$ downvotes  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"\\\"\\\"Donald Trump is still facing accountability for his st…\n$ comment_id <chr> \"2\", \"2_1\", \"2_1_1_1\", \"2_1_1_1_1\", \"2_1_1_1_1_1\", \"2_1_1_1…\n$ countid    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#how many nodes (authors)?\nlength(unique(subset_politik3$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3869\n```\n\n\n:::\n:::\n\n\nWe got 982 posts but still +3.8k nodes, it is still high number of nodes.\n\nI'll need a different approach.\n\nI'll select 2 specific posts with a \"median\" number of comments. One post will be about Trump and another about Biden, specifically I will filter posts by containing the word \"Biden\" and \"Trump\" in the title of the post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressWarnings({\n#First selecting posts with \"Trump\" or \"Biden\" included in the title of the post\n\n#Filtering the titles that contain Trump\ntrump_df <- politik_df %>% filter(grepl(\"Trump\", title))\ntrump_df$candidate <- \"Trump\"\n\n#Let's check the distribution of number of comments\npercentiles_trump <- quantile(trump_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_trump)\n\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    25%     50%     75%     90%     95%     99% \n  30.50   74.00  206.00  575.40 1052.50 2176.34 \n```\n\n\n:::\n:::\n\n\nThe median comment for Trump's post is 74 comments.Therefore I'll select the post with 74 comments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Trump\ntrump_post <- subset(trump_df, comments == 74 )\n```\n:::\n\n\nNow let's select Biden's post\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressWarnings({\n#Filtering the titles that contain Biden\nbiden_df <- politik_df %>% filter(grepl(\"Biden\", title))\nbiden_df$candidate <- \"Biden\"\n\n#Let's check the distribution of number of comments\npercentiles_biden <- quantile(biden_df$comments, probs = c(0.25, 0.50, 0.75, 0.90, 0.95, 0.99))\nprint(percentiles_biden)\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    25%     50%     75%     90%     95%     99% \n  28.00   66.50  171.25  464.50  907.75 1818.00 \n```\n\n\n:::\n:::\n\n\nMedian is 66.5 comments, let's use the post with 67 comments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Biden\nbiden_post <- subset(biden_df, comments == 67 )\n```\n:::\n\n\nNow I got the 2 main posts, let's explore a bit those 2 posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#merging the previous df's\ntrump_biden_df <- rbind(trump_post, biden_post)\nprint(trump_biden_df$url)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\"\n[2] \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" \n```\n\n\n:::\n\n```{.r .cell-code}\n#let's identify these posts in the politik3 df (containing all the details)\nsubset_politik3 <- politik_df3 %>%\n         filter(url %in% trump_biden_df$url)\n\n#creating a new column with the candidate related to the post\nsubset_politik3 <- subset_politik3 %>%\n  mutate(candidate = case_when(\n    url == \"https://www.reddit.com/r/politics/comments/1bsdho2/us_election_workers_face_thousands_of_threats_so/\" ~ \"Trump\",\n    url == \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_black_and_brown_voters_really_fleeing_biden/\" ~ \"Biden\",\n  ))\n\n#let's see the df now \nglimpse(subset_politik3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 119\nColumns: 11\n$ url        <chr> \"https://www.reddit.com/r/politics/comments/1bsnq6l/are_bla…\n$ author     <chr> \"fxkatt\", \"AngusMcTibbins\", \"Knoxcore\", \"Hattopia\", \"AngusM…\n$ date       <date> 2024-03-31, 2024-03-31, 2024-04-01, 2024-03-31, 2024-03-31…\n$ score      <int> 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ upvotes    <int> 20, 27, 3, -18, 23, -12, 18, -3, -14, 9, -7, 5, 2, 1, -4, 1…\n$ downvotes  <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ golds      <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ comment    <chr> \"It's very possible that both Biden and Trump are losing so…\n$ comment_id <chr> \"2\", \"3\", \"3_1\", \"3_2\", \"3_2_1\", \"3_2_1_1\", \"3_2_1_1_1\", \"3…\n$ countid    <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ candidate  <chr> \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Biden\", \"Bide…\n```\n\n\n:::\n\n```{.r .cell-code}\n# Let's keep only the relevant columns\npolitik_final <- select(subset_politik3, c(\"url\", \"author\", \"score\", \"comment\", \"comment_id\", \"candidate\"))\n\n# Extracting the levels of each comment and its hierarchy\npolitik_final2 <- politik_final %>%\n  mutate(Level = str_count(comment_id, pattern = \"_\") + 1,  # Count underscores to determine depth\n         ParentID = ifelse(Level > 1, sapply(strsplit(comment_id, \"_\"), function(x) paste(x[-length(x)], collapse = \"_\")), NA))\n\nlength(unique(politik_final2$author))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 80\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(unique(politik_final2$url))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\nNow we got 80 nodes (authors) from the 2 posts.\n\n#### Analysis\n\nBefore proceeding with the network analysis, let's explore a bit about the authors (users).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#let's create some tables to see frequencies and totals\n\n#first I created a count column\npolitik_final2 <- politik_final2 %>% mutate(countid = \"1\")\npolitik_final2$countid <- as.numeric(politik_final2$countid)\n\n#preparing tables\nlibrary(data.table)\npolitik_table2 <- data.table(politik_final2)\n\n#total posts grouped by author\ncount_table2 <- politik_table2 %>% group_by(author) %>% summarise(Total_posts = sum(countid))\ncount_table2 <- count_table2 %>% arrange(desc(Total_posts))\nprint(count_table2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 80 × 2\n   author            Total_posts\n   <chr>                   <dbl>\n 1 BrtFrkwr                    7\n 2 betterwoke                  5\n 3 Hattopia                    4\n 4 TheBodyPolitic1             4\n 5 TheRandomInteger            4\n 6 AngusMcTibbins              3\n 7 Due-Shirt616                3\n 8 NoDesinformatziya           3\n 9 TheReddestOrange            3\n10 DarkwingDuckHunt            2\n# ℹ 70 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary_votes <- politik_table2 %>% group_by(author) %>% summarize(Total_Score = sum(score))\nsummary_votes <- summary_votes %>% arrange(desc(Total_Score))\nprint(summary_votes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 80 × 2\n   author             Total_Score\n   <chr>                    <int>\n 1 r-m-russell                 76\n 2 AngusMcTibbins              68\n 3 OverlyComplexPants          40\n 4 BrtFrkwr                    33\n 5 betterwoke                  28\n 6 TheBodyPolitic1             24\n 7 penis_berry_crunch          22\n 8 Due-Shirt616                21\n 9 NoDesinformatziya           20\n10 YeaterdaysQuim              20\n# ℹ 70 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n#Score as a proportion of comments\nsummary_score_ratio <- politik_table2 %>% group_by(author) %>% summarize(Ratio_score_per_comment = sum(score)/sum(countid))\nsummary_score_ratio <- summary_score_ratio %>% arrange(desc(Ratio_score_per_comment))\nprint(summary_score_ratio)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 80 × 2\n   author             Ratio_score_per_comment\n   <chr>                                <dbl>\n 1 r-m-russell                           76  \n 2 OverlyComplexPants                    40  \n 3 AngusMcTibbins                        22.7\n 4 penis_berry_crunch                    22  \n 5 YeaterdaysQuim                        20  \n 6 fxkatt                                20  \n 7 grixorbatz                            17  \n 8 Sea_Engine4333                        16  \n 9 Quiet_Dimensions                      14  \n10 hdiggyh                               14  \n# ℹ 70 more rows\n```\n\n\n:::\n:::\n\n\nI would expect that nodes (users) with higher score and/or higher score per comment should be predominant (central) in the network.\n\nFor instance we got the following users in the top 5 in terms of score: \"r-m-russell\", \"AngusMcTibbins\", \"OverlyComplexPants\", \"BrtFrkwr\" and \"betterwoke\".\n\nOn the other hand here the top 5 users with highest score per comment: \"r-m-russell\", \"OverlyComplexPants\", \"AngusMcTibbins\", \"penis_berry_crunch\", and \"YeaterdaysQuim\".\n\nNow I am ready to work on this data set for the Network analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Rename level column as it represent more how deep/far is the comment\n#from the initial post, we will use this later as an attribute\npolitik_final2 <- politik_final2 %>%\n  rename(distance = Level)\n\n#identify who is commenting on the same post\npolitik_final2 <- politik_final2 %>%\n  mutate(level = substr(comment_id, 1, 2))\n\npolitik_final2$level <- str_replace_all(politik_final2$level, \"_\", \"\")\n\npolitik_final2 <- politik_final2 %>%\n  mutate(level2 = substr(candidate, 1, 1))\n\npolitik_final2$comment_id2 <- paste(politik_final2$level2, politik_final2$level, sep = \"_\")\n\n#Now I'll create a new object by keeping only the columns I need\n\npolitik_final3 <- select(politik_final2, c(-\"comment_id\", -\"ParentID\", -\"level\", -\"level2\"))\n\n\n#Will create a attribute only object to use later\npolitik_attributes <- select(politik_final3, c(\"score\", \"candidate\", \"distance\"))\n```\n:::\n\n\nI'll prepare the adjacency matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npolitik_m <- select(politik_final3, c(\"comment_id2\", \"author\"))\n\n# Identify unique names and codes\nunique_names <- unique(politik_final3$author)\nunique_codes <- unique(politik_final3$comment_id2)\n\n# Create an empty adjacency matrix\nadj_matrix <- matrix(0, nrow = length(unique_names), ncol = length(unique_names),\n                     dimnames = list(unique_names, unique_names))\n\n#Populate the adjacency matrix based on shared codes\nfor (i in 1:length(unique_names)) {\n  for (j in 1:length(unique_names)) {\n    # Check if names i and j have the same code\n    shared_code <- intersect(politik_final3$comment_id2[politik_final3$author == unique_names[i]],\n                             politik_final3$comment_id2[politik_final3$author == unique_names[j]])\n    if (length(shared_code) > 0) {\n      adj_matrix[unique_names[i], unique_names[j]] <- 1  # Set relationship to 1\n    }\n  }\n}\n\n# I'll eliminate loops in advance\ndiag(adj_matrix) <- 0\n```\n:::\n\n\n##### Network Analysis\n\nNow let's explore the Network.\n\nIt is important to mention that in this research we will assume an undirected network. We are only considering comments within the same post, not specific \"direction\" of each comment among users.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load packages\nsuppressPackageStartupMessages(library(network))\nlibrary(sna)\nlibrary(statnet)\n\npolitik.n <- network(adj_matrix, directed = FALSE)\npolitik.n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Network attributes:\n  vertices = 80 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 158 \n    missing edges= 0 \n    non-missing edges= 158 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n```\n\n\n:::\n:::\n\n\nWe got 80 nodes and 316 edges.\n\nLet's explore the network. I'll calculate the census for Dyads and triads:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Dyads and Triads census\nsna::dyad.census(politik.n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Mut Asym Null\n[1,] 158    0 3002\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(sna::triad.census(politik.n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 82160\n```\n\n\n:::\n\n```{.r .cell-code}\nsna::triad.census(politik.n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       003 012   102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210\n[1,] 70689   0 10955    0    0    0    0    0    0    0 179    0    0    0   0\n     300\n[1,] 337\n```\n\n\n:::\n:::\n\n\nIn terms of Dyads, we got 158 mutual connections and 3002 null connections.\n\nIn terms of Triads, we got 82160 triads in total: about 70k null triads, 11k open triads (one connection exist), 179 where 2 connections exist, and 337 closed triads.\n\nIt seems a disperse network, but let's see Transitivity and Density:\n\nLet's check the Transitivity coefficient\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#transitivity\ngtrans(politik.n, mode=\"graph\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8495798\n```\n\n\n:::\n:::\n\n\nThe transitivity coefficient is 0.85 which indicates a high level of cohesion.\n\nLet's see the density:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get network density: statnet\nnetwork::network.density(politik.n) #already exclude loops\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05\n```\n\n\n:::\n:::\n\n\nThis density of 0.05 indicates a relatively sparse network with few connections between nodes.\n\nThis combination of high transitivity and low density might suggests the presence of strong community structure in the network, where nodes are densely connected within their respective communities but sparsely connected between communities.\n\nLet's visualize the network\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the network\nplot(politik.n, displaylabels = TRUE, label.cex=0.7, vertex.cex=1.5, displayisolates=T, main = \"Authors Network\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nWithout isolated nodes and labels\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot the network\nplot(politik.n, displaylabels = F, label.cex=0.7, vertex.cex=1.5, displayisolates=F, main = \"Authors Network\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nLet's now include the Candidate as attribute. We will clasify the users as per their comments to Biden or Trump posts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#I'll create a column with Biden true-false attribute\npolitik_final3at <- politik_final3 %>%\n  mutate(\n    biden = if_else(candidate == \"Biden\", \"TRUE\", \"FALSE\")\n  )\n\n#now let's see how authonrs in Biden and Trump are interacting\nnodeColors<-ifelse(politik_final3at$biden,\"dodgerblue\",\"red\")\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=T, main = \"Authors Network by Candidate\") #including isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nLet's exclude isolated nodes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(politik.n,displaylabels=F,vertex.col=nodeColors,vertex.cex=1.2, displayisolates=F, main = \"Authors Network by Candidate (excluding isolated nodes)\") #excluding isolated nodes\nlegend(\"bottomright\", legend = c(\"Biden\", \"Trump\"), col = c(\"dodgerblue\", \"red\"), pch = c(21, 21), title = \"Node Type\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nIt looks like there are closed communities not connected among them. Which is consistent with the Transitivity vs Density finding before.\n\nIn particular Biden's commentors tend to be together and Trump's commentors seems more disperse.\n\nLet's see who are the authors with highest degree centrality\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a dataset of vertex names and degree: statnet\npolitik.nodes.df <- data.frame(name = politik.n %v% \"vertex.names\",\n                            degree = sna::degree(politik.n))\n\npolitik_table7 <- data.table(politik.nodes.df)\n\n#order by centrality degree\npolitik_table7 %>% arrange(desc(degree)) %>%\nslice(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  name degree\n                <char>  <num>\n 1:           Knoxcore     30\n 2:  NoDesinformatziya     30\n 3:           BrtFrkwr     28\n 4:   TheRandomInteger     26\n 5:    TheBodyPolitic1     20\n 6:         Fasefirst2     20\n 7:           cdiddy19     20\n 8: Invincible_auxcord     20\n 9:     Bulky-You-5657     20\n10:        nickmiele22     20\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(politik_table7)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     name               degree    \n Length:80          Min.   : 0.0  \n Class :character   1st Qu.: 0.0  \n Mode  :character   Median : 6.0  \n                    Mean   : 7.9  \n                    3rd Qu.:14.5  \n                    Max.   :30.0  \n```\n\n\n:::\n:::\n\n\nLet's explore the correlation between centrality degree and score (remember it is the net value between upvotes and downvotes).\n\nI am expecting a significant positive relationship between soore and degree centrality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages(library(igraph))\n\n# Create the igraph object\npolitik.ig <- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")  # Undirected by default\n\n# Calculate degree centrality for each node\ndegree_centrality <- degree(politik.ig, mode = \"all\")\n\n# If nodes do not have names, you can use node IDs\nif (is.null(V(politik.ig)$name)) {\n  V(politik.ig)$name <- as.character(1:vcount(politik.ig))\n}\n\n# Check node names\nnode_names <- V(politik.ig)$name\n\n# Create a sample data frame with some values for each node\n# Ensure the data frame has the same node identifiers as the graph\ndf <- data.frame(\n  author = node_names,  # Node names or IDs\n  value = runif(vcount(politik.ig), 1, 100)  # Random values between 1 and 100\n)\n\n# Convert degree centrality to a data frame\ndegree_centrality_df <- data.frame(\n  author = node_names,  # Node names or IDs\n  degree_centrality = degree_centrality  # Degree centrality values\n)\n\n# Merge the degree centrality data frame with the existing data frame\nmerged_df <- merge(df, degree_centrality_df, by = \"author\", all = TRUE)\nmerged_df2 <- merge(merged_df, politik_final2, by = \"author\", all = TRUE)\ndegree_scoredf <- select(merged_df2, c(\"degree_centrality\", \"score\"))\n\n# Calculate the correlation coefficient between 'score' and 'degree_centrality'\ncor_matrix1 <- cor(degree_scoredf, use = \"complete.obs\")\ncor_matrix1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  degree_centrality       score\ndegree_centrality       1.000000000 0.001921923\nscore                   0.001921923 1.000000000\n```\n\n\n:::\n:::\n\n\nSeems that the relationship between score and degree centrality is very low (0.002).\n\nSo I can't accept my hypothesis.\n\nI am curious about the top 5 users with higher degree centrality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndegree_scoredf3 <- select(merged_df2, c(\"degree_centrality\", \"author\"))\nsubset_df <- distinct(degree_scoredf3, author, .keep_all = TRUE)\n\nsubset_df %>% arrange(desc(degree_centrality))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   degree_centrality               author\n1                 15             Knoxcore\n2                 15    NoDesinformatziya\n3                 14             BrtFrkwr\n4                 13     TheRandomInteger\n5                 10       Bulky-You-5657\n6                 10             cdiddy19\n7                 10  Exciting_Slice_9492\n8                 10           Fasefirst2\n9                 10        hilljack26301\n10                10   Invincible_auxcord\n11                10             Kamelasa\n12                10          nickmiele22\n13                10      TheBodyPolitic1\n14                 8   Admirable_Bad_5649\n15                 8           betterwoke\n16                 8     DarkwingDuckHunt\n17                 8          Scarlettail\n18                 8        Spara-Extreme\n19                 8     TheReddestOrange\n20                 8     Thick-Return1694\n21                 7          bakeacake45\n22                 6            AliMcGraw\n23                 6       AngusMcTibbins\n24                 6             Hattopia\n25                 6    hellocattlecookie\n26                 6   Mundane_Rabbit7751\n27                 6   OverlyComplexPants\n28                 6         Roasted_Butt\n29                 6    SeegsonSynthetics\n30                 6             Shaunair\n31                 4             caserock\n32                 4     CecilTWashington\n33                 4            gefjunhel\n34                 4   penis_berry_crunch\n35                 4          r-m-russell\n36                 3         Due-Shirt616\n37                 3              hdiggyh\n38                 3             iuthnj34\n39                 3               JeffMo\n40                 3        PineTreeBanjo\n41                 3              Purify5\n42                 3     Quiet_Dimensions\n43                 3       YeaterdaysQuim\n44                 1  Candid_Chicken_9246\n45                 1   FijiFanBotNotGay69\n46                 1        Happypappy213\n47                 1            hindusoul\n48                 1      InGreedWeTrust3\n49                 1              LariRed\n50                 1   physical_graffitti\n51                 1            Tower6011\n52                 0              bck1999\n53                 0     bloombergopinion\n54                 0          cryolongman\n55                 0          Donut131313\n56                 0              eldred2\n57                 0      Empty-Rise-4409\n58                 0     fore_skin_walker\n59                 0               fxkatt\n60                 0           grixorbatz\n61                 0            Guttenber\n62                 0        HonoredPeople\n63                 0     ImportantNeck491\n64                 0        inconsistent3\n65                 0       JubalHarshaw23\n66                 0             njman100\n67                 0          No_Yak_6227\n68                 0       Odd_Tiger_2278\n69                 0 platanthera_ciliaris\n70                 0        PoopieButt317\n71                 0  Practical_Shop_4055\n72                 0         RUIN_NATION_\n73                 0       Sea_Engine4333\n74                 0          SeaSuch2077\n75                 0             spotspam\n76                 0          stjoechief1\n77                 0          StormOk7544\n78                 0              syg-123\n79                 0               th1961\n80                 0         Willowgirl78\n```\n\n\n:::\n:::\n\n\nUsers with higher degree centrality are \"Knoxcore\", \"NoDesinformatziya\", \"BrtFrkwr\", \"TheRandomInteger\", and \"Bulky-You-5657\"\n\nOut of these 5 nodes only one of them (\"BrtFrkwr\") is also in the top 5 related to score. So it is consistent with the low correlation between score and centrality.\n\nLet's now check what clusters (communities) we do have in this network.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run clustering algorithm: fast_greedy\npolitik.fg <- igraph::cluster_fast_greedy(politik.ig)\n# inspect clustering object\npolitik.fg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIGRAPH clustering fast greedy, groups: 37, mod: 0.63\n+ groups:\n  $`1`\n   [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n   [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n   [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n  [10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n  [13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n  [16] \"Kamelasa\"           \n  \n  $`2`\n   [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n  + ... omitted several groups/vertices\n```\n\n\n:::\n\n```{.r .cell-code}\nigraph::groups(politik.fg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1`\n [1] \"AngusMcTibbins\"      \"Knoxcore\"            \"Hattopia\"           \n [4] \"Mundane_Rabbit7751\"  \"NoDesinformatziya\"   \"SeegsonSynthetics\"  \n [7] \"hellocattlecookie\"   \"TheBodyPolitic1\"     \"Fasefirst2\"         \n[10] \"cdiddy19\"            \"Invincible_auxcord\"  \"Bulky-You-5657\"     \n[13] \"nickmiele22\"         \"Exciting_Slice_9492\" \"hilljack26301\"      \n[16] \"Kamelasa\"           \n\n$`2`\n [1] \"OverlyComplexPants\"  \"AliMcGraw\"           \"bakeacake45\"        \n [4] \"Roasted_Butt\"        \"BrtFrkwr\"            \"TheRandomInteger\"   \n [7] \"Shaunair\"            \"betterwoke\"          \"DarkwingDuckHunt\"   \n[10] \"Thick-Return1694\"    \"Spara-Extreme\"       \"Admirable_Bad_5649\" \n[13] \"TheReddestOrange\"    \"Scarlettail\"         \"Tower6011\"          \n[16] \"Candid_Chicken_9246\"\n\n$`3`\n[1] \"r-m-russell\"        \"penis_berry_crunch\" \"CecilTWashington\"  \n[4] \"caserock\"           \"gefjunhel\"         \n\n$`4`\n[1] \"iuthnj34\"       \"YeaterdaysQuim\" \"hdiggyh\"        \"Purify5\"       \n\n$`5`\n[1] \"Quiet_Dimensions\" \"Due-Shirt616\"     \"JeffMo\"           \"PineTreeBanjo\"   \n\n$`6`\n[1] \"physical_graffitti\" \"hindusoul\"         \n\n$`7`\n[1] \"LariRed\"         \"InGreedWeTrust3\"\n\n$`8`\n[1] \"Happypappy213\"      \"FijiFanBotNotGay69\"\n\n$`9`\n[1] \"fxkatt\"\n\n$`10`\n[1] \"Sea_Engine4333\"\n\n$`11`\n[1] \"Practical_Shop_4055\"\n\n$`12`\n[1] \"PoopieButt317\"\n\n$`13`\n[1] \"inconsistent3\"\n\n$`14`\n[1] \"Empty-Rise-4409\"\n\n$`15`\n[1] \"stjoechief1\"\n\n$`16`\n[1] \"th1961\"\n\n$`17`\n[1] \"platanthera_ciliaris\"\n\n$`18`\n[1] \"No_Yak_6227\"\n\n$`19`\n[1] \"fore_skin_walker\"\n\n$`20`\n[1] \"HonoredPeople\"\n\n$`21`\n[1] \"bloombergopinion\"\n\n$`22`\n[1] \"RUIN_NATION_\"\n\n$`23`\n[1] \"ImportantNeck491\"\n\n$`24`\n[1] \"grixorbatz\"\n\n$`25`\n[1] \"spotspam\"\n\n$`26`\n[1] \"JubalHarshaw23\"\n\n$`27`\n[1] \"StormOk7544\"\n\n$`28`\n[1] \"njman100\"\n\n$`29`\n[1] \"Odd_Tiger_2278\"\n\n$`30`\n[1] \"cryolongman\"\n\n$`31`\n[1] \"Willowgirl78\"\n\n$`32`\n[1] \"Guttenber\"\n\n$`33`\n[1] \"SeaSuch2077\"\n\n$`34`\n[1] \"syg-123\"\n\n$`35`\n[1] \"bck1999\"\n\n$`36`\n[1] \"Donut131313\"\n\n$`37`\n[1] \"eldred2\"\n```\n\n\n:::\n:::\n\n\nThere are 2 main clusters in the network.\n\nLet's see the density of each cluster using block model function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(blockmodel(politik.n, politik.fg$membership)$block.model,\n      digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8\nBlock 1     0.62    0.00       0       0       0       0       0       0\nBlock 2     0.00    0.48       0       0       0       0       0       0\nBlock 3     0.00    0.00       1       0       0       0       0       0\nBlock 4     0.00    0.00       0       1       0       0       0       0\nBlock 5     0.00    0.00       0       0       1       0       0       0\nBlock 6     0.00    0.00       0       0       0       1       0       0\nBlock 7     0.00    0.00       0       0       0       0       1       0\nBlock 8     0.00    0.00       0       0       0       0       0       1\nBlock 9     0.00    0.00       0       0       0       0       0       0\nBlock 10    0.00    0.00       0       0       0       0       0       0\nBlock 11    0.00    0.00       0       0       0       0       0       0\nBlock 12    0.00    0.00       0       0       0       0       0       0\nBlock 13    0.00    0.00       0       0       0       0       0       0\nBlock 14    0.00    0.00       0       0       0       0       0       0\nBlock 15    0.00    0.00       0       0       0       0       0       0\nBlock 16    0.00    0.00       0       0       0       0       0       0\nBlock 17    0.00    0.00       0       0       0       0       0       0\nBlock 18    0.00    0.00       0       0       0       0       0       0\nBlock 19    0.00    0.00       0       0       0       0       0       0\nBlock 20    0.00    0.00       0       0       0       0       0       0\nBlock 21    0.00    0.00       0       0       0       0       0       0\nBlock 22    0.00    0.00       0       0       0       0       0       0\nBlock 23    0.00    0.00       0       0       0       0       0       0\nBlock 24    0.00    0.00       0       0       0       0       0       0\nBlock 25    0.00    0.00       0       0       0       0       0       0\nBlock 26    0.00    0.00       0       0       0       0       0       0\nBlock 27    0.00    0.00       0       0       0       0       0       0\nBlock 28    0.00    0.00       0       0       0       0       0       0\nBlock 29    0.00    0.00       0       0       0       0       0       0\nBlock 30    0.00    0.00       0       0       0       0       0       0\nBlock 31    0.00    0.00       0       0       0       0       0       0\nBlock 32    0.00    0.00       0       0       0       0       0       0\nBlock 33    0.00    0.00       0       0       0       0       0       0\nBlock 34    0.00    0.00       0       0       0       0       0       0\nBlock 35    0.00    0.00       0       0       0       0       0       0\nBlock 36    0.00    0.00       0       0       0       0       0       0\nBlock 37    0.00    0.00       0       0       0       0       0       0\n         Block 9 Block 10 Block 11 Block 12 Block 13 Block 14 Block 15 Block 16\nBlock 1        0        0        0        0        0        0        0        0\nBlock 2        0        0        0        0        0        0        0        0\nBlock 3        0        0        0        0        0        0        0        0\nBlock 4        0        0        0        0        0        0        0        0\nBlock 5        0        0        0        0        0        0        0        0\nBlock 6        0        0        0        0        0        0        0        0\nBlock 7        0        0        0        0        0        0        0        0\nBlock 8        0        0        0        0        0        0        0        0\nBlock 9      NaN        0        0        0        0        0        0        0\nBlock 10       0      NaN        0        0        0        0        0        0\nBlock 11       0        0      NaN        0        0        0        0        0\nBlock 12       0        0        0      NaN        0        0        0        0\nBlock 13       0        0        0        0      NaN        0        0        0\nBlock 14       0        0        0        0        0      NaN        0        0\nBlock 15       0        0        0        0        0        0      NaN        0\nBlock 16       0        0        0        0        0        0        0      NaN\nBlock 17       0        0        0        0        0        0        0        0\nBlock 18       0        0        0        0        0        0        0        0\nBlock 19       0        0        0        0        0        0        0        0\nBlock 20       0        0        0        0        0        0        0        0\nBlock 21       0        0        0        0        0        0        0        0\nBlock 22       0        0        0        0        0        0        0        0\nBlock 23       0        0        0        0        0        0        0        0\nBlock 24       0        0        0        0        0        0        0        0\nBlock 25       0        0        0        0        0        0        0        0\nBlock 26       0        0        0        0        0        0        0        0\nBlock 27       0        0        0        0        0        0        0        0\nBlock 28       0        0        0        0        0        0        0        0\nBlock 29       0        0        0        0        0        0        0        0\nBlock 30       0        0        0        0        0        0        0        0\nBlock 31       0        0        0        0        0        0        0        0\nBlock 32       0        0        0        0        0        0        0        0\nBlock 33       0        0        0        0        0        0        0        0\nBlock 34       0        0        0        0        0        0        0        0\nBlock 35       0        0        0        0        0        0        0        0\nBlock 36       0        0        0        0        0        0        0        0\nBlock 37       0        0        0        0        0        0        0        0\n         Block 17 Block 18 Block 19 Block 20 Block 21 Block 22 Block 23\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17      NaN        0        0        0        0        0        0\nBlock 18        0      NaN        0        0        0        0        0\nBlock 19        0        0      NaN        0        0        0        0\nBlock 20        0        0        0      NaN        0        0        0\nBlock 21        0        0        0        0      NaN        0        0\nBlock 22        0        0        0        0        0      NaN        0\nBlock 23        0        0        0        0        0        0      NaN\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 24 Block 25 Block 26 Block 27 Block 28 Block 29 Block 30\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24      NaN        0        0        0        0        0        0\nBlock 25        0      NaN        0        0        0        0        0\nBlock 26        0        0      NaN        0        0        0        0\nBlock 27        0        0        0      NaN        0        0        0\nBlock 28        0        0        0        0      NaN        0        0\nBlock 29        0        0        0        0        0      NaN        0\nBlock 30        0        0        0        0        0        0      NaN\nBlock 31        0        0        0        0        0        0        0\nBlock 32        0        0        0        0        0        0        0\nBlock 33        0        0        0        0        0        0        0\nBlock 34        0        0        0        0        0        0        0\nBlock 35        0        0        0        0        0        0        0\nBlock 36        0        0        0        0        0        0        0\nBlock 37        0        0        0        0        0        0        0\n         Block 31 Block 32 Block 33 Block 34 Block 35 Block 36 Block 37\nBlock 1         0        0        0        0        0        0        0\nBlock 2         0        0        0        0        0        0        0\nBlock 3         0        0        0        0        0        0        0\nBlock 4         0        0        0        0        0        0        0\nBlock 5         0        0        0        0        0        0        0\nBlock 6         0        0        0        0        0        0        0\nBlock 7         0        0        0        0        0        0        0\nBlock 8         0        0        0        0        0        0        0\nBlock 9         0        0        0        0        0        0        0\nBlock 10        0        0        0        0        0        0        0\nBlock 11        0        0        0        0        0        0        0\nBlock 12        0        0        0        0        0        0        0\nBlock 13        0        0        0        0        0        0        0\nBlock 14        0        0        0        0        0        0        0\nBlock 15        0        0        0        0        0        0        0\nBlock 16        0        0        0        0        0        0        0\nBlock 17        0        0        0        0        0        0        0\nBlock 18        0        0        0        0        0        0        0\nBlock 19        0        0        0        0        0        0        0\nBlock 20        0        0        0        0        0        0        0\nBlock 21        0        0        0        0        0        0        0\nBlock 22        0        0        0        0        0        0        0\nBlock 23        0        0        0        0        0        0        0\nBlock 24        0        0        0        0        0        0        0\nBlock 25        0        0        0        0        0        0        0\nBlock 26        0        0        0        0        0        0        0\nBlock 27        0        0        0        0        0        0        0\nBlock 28        0        0        0        0        0        0        0\nBlock 29        0        0        0        0        0        0        0\nBlock 30        0        0        0        0        0        0        0\nBlock 31      NaN        0        0        0        0        0        0\nBlock 32        0      NaN        0        0        0        0        0\nBlock 33        0        0      NaN        0        0        0        0\nBlock 34        0        0        0      NaN        0        0        0\nBlock 35        0        0        0        0      NaN        0        0\nBlock 36        0        0        0        0        0      NaN        0\nBlock 37        0        0        0        0        0        0      NaN\n```\n\n\n:::\n:::\n\n\nThe blocks 1 and 2 in our network seems dense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_comm <- data.frame(\n  Node = V(politik.ig)$name,\n  Community = politik.fg$membership,\n  Degree = degree_centrality\n)\n\n# 4. Find maximum degree for each community\n\nhighest_degree_nodes <- df_comm %>%\n  group_by(Community) %>%\n  filter(Degree == max(Degree)) %>%\n  ungroup()\n\nhighest_degree_nodes %>% arrange(-desc(Community))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 51 × 3\n   Node               Community Degree\n   <chr>                  <dbl>  <dbl>\n 1 Knoxcore                   1     15\n 2 NoDesinformatziya          1     15\n 3 BrtFrkwr                   2     14\n 4 r-m-russell                3      4\n 5 penis_berry_crunch         3      4\n 6 CecilTWashington           3      4\n 7 caserock                   3      4\n 8 gefjunhel                  3      4\n 9 iuthnj34                   4      3\n10 YeaterdaysQuim             4      3\n# ℹ 41 more rows\n```\n\n\n:::\n:::\n\n\nLet's now visualize the communities including the nodes with highest degree centrality for the main communities (1 and 2):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify nodes with high degree centrality (e.g., top 3)\ntop_nodes <- names(sort(degree_centrality, decreasing = TRUE))[1:5]\n\n# Create a custom labeling vector\nnode_labels <- rep(NA, vcount(politik.ig))\n# Set the labels for the nodes with high degree centrality\nnode_labels[top_nodes] <- top_nodes\n\n# Identify isolated nodes\nisolated_nodes <- which(degree(politik.ig) == 0)\n\n# Remove isolated nodes from the network\nnetwork_no_isolated <- delete_vertices(politik.ig, isolated_nodes)\n\nplot(network_no_isolated,\n     vertex.label = node_labels,  \n     vertex.label.cex = 0.8,\n     vertex.color = membership(politik.fg),\n     vertex.label.color = \"black\",\n     vertex.shape = \"sphere\",\n     layout = layout_with_fr,\n     main = \"Communities without isolated nodes and labeling top 5 central nodes\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){width=1056}\n:::\n:::\n\n\nIt looks like while we have 2 main communities the nodes with highest degree centrality are mostly in one of the communities.\n\nIt seems like that group of authors are close among them and at the same time are central in the network.\n\n##### Sentiment Analysis and the Network\n\nLastly let's add a new attribute to the network using text analysis, in particular sentiment analysis.\n\nLet's get the sentiment for each author's comments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sentimentr)\n\n#labeling based on the sentiment score\ncomments <- politik_final3$comment\nget_sentiment_label <- function(ave_sentiment) {\n  if (ave_sentiment > 0.1) {\n    return(\"Positive\")\n  } else if (ave_sentiment < -0.1) {\n    return(\"Negative\")\n  } else {\n    return(\"Neutral\")\n  }\n}\nsentiment_scores <- sentiment_by(x = comments, text.var = comments)\n\n#adding the label for each author in the data set\npolitik_final3$sentiment <- sapply(sentiment_scores$ave_sentiment, get_sentiment_label)\n```\n:::\n\n\nNow let's visualize the network by sentiment of each node.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a graph object from the data frame\ng <- graph_from_data_frame(politik.n, directed = FALSE)\n\n# Add node attributes to the graph\nV(g)$sentiment <- politik_final3$sentiment[match(V(g)$name, politik_final3$author)]\n\n\n# Define color palette for categories\ncolor_palette <- c(\"Negative\" = \"red\", \"Neutral\" = \"blue\", \"Positive\" = \"lightgreen\")\n\n# Visualize the network with colored nodes\nplot(g, vertex.color = color_palette[V(g)$sentiment], layout = layout_nicely, vertex.label = NA, vertex.size = 7, isolates=TRUE, main = \"Authors Network by Sentiment\")\nlegend(\"bottomright\", legend = c(\"Positive\", \"Neutral\", \"Negative\"), col = c(\"lightgreen\", \"blue\", \"red\"), pch = c(21, 21), title = \"Node Sentiment\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\nFrom this chart it looks like:\n\n1.  Most of the sentiments are either neutral or negative\n2.  Sentiments tend to get closer, or group among them.\n\n### Conclusion\n\n1.  The network was found to be sparse (low density) but highly interconnected within subgroups (high transitivity), suggesting the presence of distinct communities.\n\n2.  The visualization of the network confirmed the presence of communities, showing separate clusters of users commenting on Biden and Trump posts. The network was divided into 37 distinct clusters, indicating multiple smaller communities within the larger Biden and Trump-focused groups.\n\n3.  The hypothesis that users with higher scores (more upvotes or higher score) would also have more central positions in the network was not supported. The correlation between score and centrality was negligible.\n\n4.  While score wasn't a strong indicator of influence, degree centrality (number of connections) was used to identify the most connected users. These users may play important roles in shaping discussions within their respective communities.\n\n5.  In terms of the sentiment analysis it seems that most comments were neutral or negative. And, users with similar sentiments tended to interact with each other.\n\nThis Network Analysis research offers interesting insights into the social dynamics of a politically charged online community. It highlights the presence of distinct communities, the complex interplay between user engagement (score) and influence (centrality), and the opportunity for further exploration of the factors shaping political discourse on platforms like Reddit.\n\n### References\n\nBail C. A. (2016). Combining natural language processing and network analysis to examine how advocacy organizations stimulate conversation on social media. Proceedings of the National Academy of Sciences of the United States of America, 113(42), 11823--11828. https://doi.org/10.1073/pnas.1607151113\n\nConover, M., Ratkiewicz, J., Francisco, M., Goncalves, B., Menczer, F., & Flammini, A. (2021). Political Polarization on Twitter. Proceedings of the International AAAI Conference on Web and Social Media, 5(1), 89-96. https://doi.org/10.1609/icwsm.v5i1.14126\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}